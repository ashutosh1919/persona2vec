{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd32f20-f0f2-47c2-9786-51dc1dc0dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid, KarateClub\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.loss\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3131901-cc81-4a9f-96f1-ee7e0e3d45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, dataset_cls, dataset_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device)\n",
    "    ])\n",
    "    obj = dataset_cls(transform=transform) # dataset_cls(data_dir, dataset_name, transform=transform)\n",
    "    network = to_networkx(obj[0])\n",
    "    return nx.adjacency_matrix(network), obj[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e84707-7e3c-4ef0-a643-43c9c7fb29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj, features = load_dataset(\n",
    "#     osp.join('data', 'Planetoid'),\n",
    "#     Planetoid,\n",
    "#     'Cora'\n",
    "# )\n",
    "\n",
    "adj, features = load_dataset(\n",
    "    osp.join('data', 'KarateClub'),\n",
    "    KarateClub,\n",
    "    'KarateClub'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e375f5ed-5793-4702-9c62-ae52e27e68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Adj. Matrix: (34, 34)\n",
      "Shape of features: (34, 34)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Adj. Matrix: {}\".format(adj.shape))\n",
    "print(\"Shape of features: {}\".format(tuple(features.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fb7e3e-6428-4514-ade2-296ab125de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0., act=F.relu):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input_tensor, adj):\n",
    "        input_tensor = F.dropout(input_tensor, self.dropout, self.training)\n",
    "        support = torch.mm(input_tensor, self.weight)\n",
    "        output_tensor = torch.spmm(adj, support)\n",
    "        output_tensor = self.act(output_tensor)\n",
    "        return output_tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610a3545-32f0-4220-bbb2-42f7cfacdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, labels, mu, logvar, n_nodes, norm, pos_weight):\n",
    "    print(type(pos_weight))\n",
    "    cost = norm * F.binary_cross_entropy_with_logits(preds, labels, pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 / n_nodes * torch.mean(torch.sum(\n",
    "        1 + 2 * logvar - mu.pow(2) - logvar.exp().pow(2), 1))\n",
    "    return cost + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bcb4490-0d68-461e-b6da-3c2cc97a2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(Module):\n",
    "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout, act=torch.sigmoid):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.dropout(z, self.dropout, training=self.training)\n",
    "        adj = self.act(torch.mm(z, z.t()))\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb9cacf-da0f-4a71-9aec-e80cc16e94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModelVAE(Module):\n",
    "    def __init__(self, input_feat_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "        super(GCNModelVAE, self).__init__()\n",
    "        self.gc1 = GCNLayer(input_feat_dim, hidden_dim1, dropout, act=F.relu)\n",
    "        self.gc2 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.gc3 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.dc = InnerProductDecoder(dropout, act=lambda x: x)\n",
    "\n",
    "    def encode(self, x, adj):\n",
    "        hidden1 = self.gc1(x, adj)\n",
    "        return self.gc2(hidden1, adj), self.gc3(hidden1, adj)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        mu, logvar = self.encode(x, adj)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.dc(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0813be-93a0-4e77-b7fb-5ea27e912094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cbfd257-6523-4abf-ad8c-f1f7e07ca0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    \"\"\"\n",
    "    Function to build test set with 10% positive links\n",
    "    \"\"\"\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "    \n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "    \n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "    \n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "    \n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "def get_roc_score(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6de504c0-af1f-4ad4-9fa2-11fd4888e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed' : 42, \n",
    "    'epochs' : 200,\n",
    "    'hidden1' : 8,\n",
    "    'hidden2' : 4,\n",
    "    'lr' : 0.01,\n",
    "    'dropout' : 0.,\n",
    "    'dataset_str' : 'KarateClub'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "169431df-0d67-4afe-a269-7c61dfabebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    print(\"Using {} dataset\".format(args['dataset_str']))\n",
    "    # adj, features = load_dataset(\n",
    "    #     osp.join('data', 'Planetoid'),\n",
    "    #     Planetoid,\n",
    "    #     args['dataset_str']\n",
    "    # )\n",
    "    adj, features = load_dataset(\n",
    "        osp.join('data', 'KarateClub'),\n",
    "        KarateClub,\n",
    "        'KarateClub'\n",
    "    )\n",
    "    n_nodes, feat_dim = tuple(features.shape)\n",
    "\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "    \n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "    adj = adj_train\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    # adj_label = sparse_to_tuple(adj_label)\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    \n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "    model = GCNModelVAE(feat_dim, args['hidden1'], args['hidden2'], args['dropout'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    \n",
    "    hidden_emb = None\n",
    "    for epoch in range(args['epochs']):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        recovered, mu, logvar = model(features, adj_norm)\n",
    "        loss = loss_function(preds=recovered, labels=adj_label,\n",
    "                             mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                             norm=norm, pos_weight=pos_weight)\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        hidden_emb = mu.data.numpy()\n",
    "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "              \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "              \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "              )\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "021b753b-b877-4103-b128-23163d7ab71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KarateClub dataset\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0001 train_loss= 1.20458 val_ap= 0.75556 time= 0.00567\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0002 train_loss= 1.11300 val_ap= 0.75556 time= 0.00406\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0003 train_loss= 1.00502 val_ap= 0.75556 time= 0.00310\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0004 train_loss= 0.98628 val_ap= 0.75556 time= 0.00315\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0005 train_loss= 1.07158 val_ap= 0.75556 time= 0.00331\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0006 train_loss= 1.07066 val_ap= 0.75556 time= 0.00333\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0007 train_loss= 1.21783 val_ap= 0.75556 time= 0.00317\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0008 train_loss= 1.09525 val_ap= 0.75556 time= 0.00327\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0009 train_loss= 1.08576 val_ap= 0.75556 time= 0.00329\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0010 train_loss= 0.98119 val_ap= 0.75556 time= 0.00387\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0011 train_loss= 0.92431 val_ap= 0.75556 time= 0.00261\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0012 train_loss= 1.20121 val_ap= 0.70000 time= 0.00251\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0013 train_loss= 1.12683 val_ap= 0.70000 time= 0.00241\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0014 train_loss= 0.95811 val_ap= 0.70000 time= 0.00274\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0015 train_loss= 1.01807 val_ap= 0.75556 time= 0.00219\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0016 train_loss= 0.93102 val_ap= 0.58889 time= 0.00202\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0017 train_loss= 0.92960 val_ap= 0.58889 time= 0.00216\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0018 train_loss= 0.98592 val_ap= 0.53333 time= 0.00198\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0019 train_loss= 0.99915 val_ap= 0.53333 time= 0.00210\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0020 train_loss= 0.97473 val_ap= 0.53333 time= 0.00195\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0021 train_loss= 0.87819 val_ap= 0.53333 time= 0.00187\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0022 train_loss= 0.93671 val_ap= 0.53333 time= 0.00192\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0023 train_loss= 0.81218 val_ap= 0.53333 time= 0.00192\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0024 train_loss= 1.25188 val_ap= 0.53333 time= 0.00174\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0025 train_loss= 1.06936 val_ap= 0.53333 time= 0.00183\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0026 train_loss= 0.97034 val_ap= 0.53333 time= 0.00174\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0027 train_loss= 0.94148 val_ap= 0.53333 time= 0.00166\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0028 train_loss= 1.00301 val_ap= 0.53333 time= 0.00168\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0029 train_loss= 1.00698 val_ap= 0.53333 time= 0.00158\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0030 train_loss= 0.94541 val_ap= 0.53333 time= 0.00157\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0031 train_loss= 0.85631 val_ap= 0.53333 time= 0.00152\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0032 train_loss= 0.88316 val_ap= 0.53333 time= 0.00169\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0033 train_loss= 0.93252 val_ap= 0.53333 time= 0.00154\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0034 train_loss= 0.84680 val_ap= 0.53333 time= 0.00140\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0035 train_loss= 0.87942 val_ap= 0.53333 time= 0.00144\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0036 train_loss= 1.06856 val_ap= 0.53333 time= 0.00127\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0037 train_loss= 0.91100 val_ap= 0.44444 time= 0.00143\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0038 train_loss= 0.87062 val_ap= 0.44444 time= 0.00122\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0039 train_loss= 0.93472 val_ap= 0.44444 time= 0.00131\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0040 train_loss= 0.97384 val_ap= 0.44444 time= 0.00123\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0041 train_loss= 0.91369 val_ap= 0.44444 time= 0.00132\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0042 train_loss= 0.84593 val_ap= 0.44444 time= 0.00138\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0043 train_loss= 1.01328 val_ap= 0.44444 time= 0.00126\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0044 train_loss= 0.89851 val_ap= 0.50000 time= 0.00115\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0045 train_loss= 0.84043 val_ap= 0.50000 time= 0.00124\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0046 train_loss= 0.90384 val_ap= 0.50000 time= 0.00126\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0047 train_loss= 0.91410 val_ap= 0.50000 time= 0.00135\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0048 train_loss= 0.86493 val_ap= 0.50000 time= 0.00142\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0049 train_loss= 0.85785 val_ap= 0.50000 time= 0.00126\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0050 train_loss= 0.85373 val_ap= 0.50000 time= 0.00126\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0051 train_loss= 0.84783 val_ap= 0.50000 time= 0.00121\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0052 train_loss= 0.82740 val_ap= 0.50000 time= 0.00130\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0053 train_loss= 0.79568 val_ap= 0.50000 time= 0.00115\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0054 train_loss= 0.79192 val_ap= 0.50000 time= 0.00147\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0055 train_loss= 0.80744 val_ap= 0.50000 time= 0.00160\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0056 train_loss= 0.79545 val_ap= 0.50000 time= 0.00148\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0057 train_loss= 0.80144 val_ap= 0.66667 time= 0.00132\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0058 train_loss= 0.74641 val_ap= 0.66667 time= 0.00144\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0059 train_loss= 0.77856 val_ap= 0.66667 time= 0.00146\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0060 train_loss= 0.74503 val_ap= 0.66667 time= 0.00127\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0061 train_loss= 0.77768 val_ap= 0.66667 time= 0.00128\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0062 train_loss= 0.80010 val_ap= 0.66667 time= 0.00127\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0063 train_loss= 0.77285 val_ap= 0.66667 time= 0.00122\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0064 train_loss= 0.79298 val_ap= 0.66667 time= 0.00104\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0065 train_loss= 0.81527 val_ap= 0.66667 time= 0.00119\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0066 train_loss= 0.79163 val_ap= 0.66667 time= 0.00123\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0067 train_loss= 0.78008 val_ap= 0.66667 time= 0.00133\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0068 train_loss= 0.73321 val_ap= 0.66667 time= 0.00218\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0069 train_loss= 0.80344 val_ap= 0.63333 time= 0.00142\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0070 train_loss= 0.79119 val_ap= 0.63333 time= 0.00134\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0071 train_loss= 0.75812 val_ap= 0.66667 time= 0.00125\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0072 train_loss= 0.81872 val_ap= 0.63333 time= 0.00129\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0073 train_loss= 0.77785 val_ap= 0.66667 time= 0.00144\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0074 train_loss= 0.80330 val_ap= 0.66667 time= 0.00144\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0075 train_loss= 0.72353 val_ap= 0.66667 time= 0.00127\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0076 train_loss= 0.74928 val_ap= 0.66667 time= 0.00125\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0077 train_loss= 0.76804 val_ap= 0.66667 time= 0.00132\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0078 train_loss= 0.76120 val_ap= 0.66667 time= 0.00115\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0079 train_loss= 0.77476 val_ap= 0.66667 time= 0.00130\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0080 train_loss= 0.74634 val_ap= 0.66667 time= 0.00121\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0081 train_loss= 0.80284 val_ap= 0.66667 time= 0.00123\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0082 train_loss= 0.79462 val_ap= 0.66667 time= 0.00110\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0083 train_loss= 0.77203 val_ap= 0.66667 time= 0.00126\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0084 train_loss= 0.76313 val_ap= 0.66667 time= 0.00106\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0085 train_loss= 0.78793 val_ap= 0.66667 time= 0.00141\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0086 train_loss= 0.72431 val_ap= 0.66667 time= 0.00138\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0087 train_loss= 0.76670 val_ap= 0.66667 time= 0.00117\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0088 train_loss= 0.75259 val_ap= 0.66667 time= 0.00117\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0089 train_loss= 0.74449 val_ap= 0.66667 time= 0.00115\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0090 train_loss= 0.76864 val_ap= 0.66667 time= 0.00120\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0091 train_loss= 0.82070 val_ap= 0.66667 time= 0.00120\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0092 train_loss= 0.78063 val_ap= 0.66667 time= 0.00115\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0093 train_loss= 0.72746 val_ap= 0.66667 time= 0.00130\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0094 train_loss= 0.77436 val_ap= 0.70000 time= 0.00122\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0095 train_loss= 0.74774 val_ap= 0.70000 time= 0.00125\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0096 train_loss= 0.79446 val_ap= 0.70000 time= 0.00164\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0097 train_loss= 0.75030 val_ap= 0.70000 time= 0.00122\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0098 train_loss= 0.75716 val_ap= 0.70000 time= 0.00108\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0099 train_loss= 0.75525 val_ap= 0.70000 time= 0.00141\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0100 train_loss= 0.76918 val_ap= 0.70000 time= 0.00126\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0101 train_loss= 0.76681 val_ap= 0.70000 time= 0.00136\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0102 train_loss= 0.78217 val_ap= 0.70000 time= 0.00138\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0103 train_loss= 0.74286 val_ap= 0.70000 time= 0.00142\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0104 train_loss= 0.77177 val_ap= 0.70000 time= 0.00119\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0105 train_loss= 0.76337 val_ap= 0.75556 time= 0.00122\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0106 train_loss= 0.80780 val_ap= 0.75556 time= 0.00123\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0107 train_loss= 0.75975 val_ap= 0.75556 time= 0.00137\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0108 train_loss= 0.79537 val_ap= 0.80556 time= 0.00119\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0109 train_loss= 0.74926 val_ap= 0.80556 time= 0.00137\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0110 train_loss= 0.72527 val_ap= 0.80556 time= 0.00206\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0111 train_loss= 0.76152 val_ap= 0.80556 time= 0.00158\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0112 train_loss= 0.76076 val_ap= 0.80556 time= 0.00190\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0113 train_loss= 0.71762 val_ap= 0.80556 time= 0.00196\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0114 train_loss= 0.72750 val_ap= 0.80556 time= 0.00148\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0115 train_loss= 0.71463 val_ap= 0.80556 time= 0.00194\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0116 train_loss= 0.77519 val_ap= 0.80556 time= 0.00223\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0117 train_loss= 0.75176 val_ap= 0.80556 time= 0.00316\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0118 train_loss= 0.72708 val_ap= 0.80556 time= 0.00263\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0119 train_loss= 0.75270 val_ap= 0.80556 time= 0.00213\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0120 train_loss= 0.71429 val_ap= 0.80556 time= 0.00179\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0121 train_loss= 0.74098 val_ap= 0.80556 time= 0.00197\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0122 train_loss= 0.70657 val_ap= 0.80556 time= 0.00166\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0123 train_loss= 0.69622 val_ap= 0.80556 time= 0.00179\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0124 train_loss= 0.73662 val_ap= 0.80556 time= 0.00165\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0125 train_loss= 0.69589 val_ap= 0.80556 time= 0.00149\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0126 train_loss= 0.74932 val_ap= 0.80556 time= 0.00154\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0127 train_loss= 0.71965 val_ap= 0.80556 time= 0.00202\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0128 train_loss= 0.72830 val_ap= 0.80556 time= 0.00200\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0129 train_loss= 0.70429 val_ap= 0.80556 time= 0.00148\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0130 train_loss= 0.68840 val_ap= 0.80556 time= 0.00156\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0131 train_loss= 0.72991 val_ap= 0.80556 time= 0.00168\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0132 train_loss= 0.70421 val_ap= 0.80556 time= 0.00168\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0133 train_loss= 0.64406 val_ap= 0.80556 time= 0.00159\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0134 train_loss= 0.69041 val_ap= 0.80556 time= 0.00181\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0135 train_loss= 0.69068 val_ap= 0.80556 time= 0.00181\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0136 train_loss= 0.69001 val_ap= 0.80556 time= 0.00158\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0137 train_loss= 0.69511 val_ap= 0.80556 time= 0.00148\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0138 train_loss= 0.66117 val_ap= 0.80556 time= 0.00163\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0139 train_loss= 0.72569 val_ap= 0.63889 time= 0.00170\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0140 train_loss= 0.70473 val_ap= 0.63889 time= 0.00150\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0141 train_loss= 0.69403 val_ap= 0.63889 time= 0.00157\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0142 train_loss= 0.67863 val_ap= 0.63889 time= 0.00192\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0143 train_loss= 0.64309 val_ap= 0.63889 time= 0.00144\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0144 train_loss= 0.68400 val_ap= 0.63889 time= 0.00167\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0145 train_loss= 0.66900 val_ap= 0.63889 time= 0.00169\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0146 train_loss= 0.66573 val_ap= 0.80556 time= 0.00127\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0147 train_loss= 0.65829 val_ap= 0.80556 time= 0.00147\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0148 train_loss= 0.66788 val_ap= 0.80556 time= 0.00162\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0149 train_loss= 0.65068 val_ap= 0.80556 time= 0.00161\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0150 train_loss= 0.63174 val_ap= 0.80556 time= 0.00185\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0151 train_loss= 0.64279 val_ap= 0.80556 time= 0.00174\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0152 train_loss= 0.64181 val_ap= 0.80556 time= 0.00195\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0153 train_loss= 0.65450 val_ap= 0.80556 time= 0.00159\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0154 train_loss= 0.67030 val_ap= 0.80556 time= 0.00173\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0155 train_loss= 0.61943 val_ap= 0.80556 time= 0.00198\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0156 train_loss= 0.65892 val_ap= 0.80556 time= 0.00140\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0157 train_loss= 0.64773 val_ap= 0.80556 time= 0.00160\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0158 train_loss= 0.64298 val_ap= 0.80556 time= 0.00143\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0159 train_loss= 0.65288 val_ap= 0.80556 time= 0.00164\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0160 train_loss= 0.65053 val_ap= 0.80556 time= 0.00154\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0161 train_loss= 0.58716 val_ap= 0.80556 time= 0.00150\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0162 train_loss= 0.61213 val_ap= 0.80556 time= 0.00145\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0163 train_loss= 0.60266 val_ap= 0.80556 time= 0.00168\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0164 train_loss= 0.60933 val_ap= 0.80556 time= 0.00159\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0165 train_loss= 0.61975 val_ap= 0.80556 time= 0.00151\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0166 train_loss= 0.63878 val_ap= 0.80556 time= 0.00193\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0167 train_loss= 0.61728 val_ap= 0.80556 time= 0.00158\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0168 train_loss= 0.62477 val_ap= 0.80556 time= 0.00146\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0169 train_loss= 0.60895 val_ap= 0.80556 time= 0.00183\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0170 train_loss= 0.59596 val_ap= 0.80556 time= 0.00204\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0171 train_loss= 0.60403 val_ap= 0.80556 time= 0.00174\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0172 train_loss= 0.63845 val_ap= 0.80556 time= 0.00174\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0173 train_loss= 0.61914 val_ap= 0.80556 time= 0.00145\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0174 train_loss= 0.61261 val_ap= 0.80556 time= 0.00137\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0175 train_loss= 0.61104 val_ap= 0.80556 time= 0.00140\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0176 train_loss= 0.58859 val_ap= 0.80556 time= 0.00187\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0177 train_loss= 0.60113 val_ap= 0.80556 time= 0.00177\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0178 train_loss= 0.60528 val_ap= 0.80556 time= 0.00154\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0179 train_loss= 0.60358 val_ap= 0.80556 time= 0.00149\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0180 train_loss= 0.63064 val_ap= 0.80556 time= 0.00136\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0181 train_loss= 0.61938 val_ap= 0.80556 time= 0.00178\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0182 train_loss= 0.60440 val_ap= 0.80556 time= 0.00167\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0183 train_loss= 0.61451 val_ap= 0.80556 time= 0.00151\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0184 train_loss= 0.60076 val_ap= 0.80556 time= 0.00181\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0185 train_loss= 0.62426 val_ap= 0.80556 time= 0.00141\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0186 train_loss= 0.59722 val_ap= 0.80556 time= 0.00171\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0187 train_loss= 0.60760 val_ap= 0.80556 time= 0.00190\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0188 train_loss= 0.60052 val_ap= 0.80556 time= 0.00139\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0189 train_loss= 0.61580 val_ap= 0.80556 time= 0.00146\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0190 train_loss= 0.63681 val_ap= 0.80556 time= 0.00197\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0191 train_loss= 0.60030 val_ap= 0.80556 time= 0.00161\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0192 train_loss= 0.61188 val_ap= 0.80556 time= 0.00153\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0193 train_loss= 0.60732 val_ap= 0.80556 time= 0.00167\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0194 train_loss= 0.60661 val_ap= 0.80556 time= 0.00155\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0195 train_loss= 0.64160 val_ap= 0.80556 time= 0.00137\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0196 train_loss= 0.62647 val_ap= 0.80556 time= 0.00151\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0197 train_loss= 0.61675 val_ap= 0.80556 time= 0.00155\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0198 train_loss= 0.62972 val_ap= 0.80556 time= 0.00171\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0199 train_loss= 0.62682 val_ap= 0.80556 time= 0.00157\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0200 train_loss= 0.60586 val_ap= 0.80556 time= 0.00185\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9183673469387755\n",
      "Test AP score: 0.9240362811791383\n"
     ]
    }
   ],
   "source": [
    "model = build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7af4e-9110-4db1-b85d-e9ea44e79551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona2vec",
   "language": "python",
   "name": "persona2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
