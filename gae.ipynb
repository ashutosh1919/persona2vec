{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2289997e-3125-4b93-afea-f3b410144552",
   "metadata": {},
   "source": [
    "## Exp 1: VGAE using GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5dd32f20-f0f2-47c2-9786-51dc1dc0dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid, KarateClub\n",
    "from torch_geometric.nn import GAE, VGAE, DenseGCNConv\n",
    "from torch_geometric.utils import to_networkx, to_dense_adj, dense_to_sparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.loss\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f3131901-cc81-4a9f-96f1-ee7e0e3d45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, dataset_cls, dataset_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device)\n",
    "    ])\n",
    "    obj = dataset_cls(data_dir, dataset_name, transform=transform)\n",
    "    network = to_networkx(obj[0])\n",
    "    return nx.adjacency_matrix(network), obj[0].x\n",
    "\n",
    "def load_dataset_pytorch(data_dir, dataset_cls, dataset_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device)\n",
    "    ])\n",
    "    obj = dataset_cls(data_dir, dataset_name, transform=transform)\n",
    "    return obj[0].x, obj[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13e84707-7e3c-4ef0-a643-43c9c7fb29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features = load_dataset(\n",
    "    osp.join('data', 'Planetoid'),\n",
    "    Planetoid,\n",
    "    'Cora'\n",
    ")\n",
    "\n",
    "# adj, features = load_dataset(\n",
    "#     osp.join('data', 'KarateClub'),\n",
    "#     KarateClub,\n",
    "#     'KarateClub'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ce9dd27-77dc-483d-a3c0-1958347f3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ccee4bd-9629-4315-aa57-62777ca7700e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sp.coo_matrix(adj_train).shape\n",
    "# train_edges.transpose().shape\n",
    "# val_edges.transpose().shape\n",
    "indices, ad = preprocess_graph(adj)\n",
    "ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61082b84-2a75-45ae-a668-0ff7cef1ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device)\n",
    "])\n",
    "obj = Planetoid(osp.join('data', 'Planetoid'), 'Cora', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d5d78bb-ceed-4b5a-be89-f14d792b96c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dense_adj(obj[0].edge_index)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e375f5ed-5793-4702-9c62-ae52e27e68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Adj. Matrix: (2708, 2708)\n",
      "Shape of features: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Adj. Matrix: {}\".format(adj.shape))\n",
    "print(\"Shape of features: {}\".format(tuple(features.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3fb7e3e-6428-4514-ade2-296ab125de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0., act=F.relu):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input_tensor, adj):\n",
    "        input_tensor = F.dropout(input_tensor, self.dropout, self.training)\n",
    "        support = torch.mm(input_tensor, self.weight)\n",
    "        output_tensor = torch.spmm(adj, support)\n",
    "        output_tensor = self.act(output_tensor)\n",
    "        return output_tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "610a3545-32f0-4220-bbb2-42f7cfacdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, labels, mu, logvar, n_nodes, norm, pos_weight):\n",
    "    print(type(pos_weight))\n",
    "    cost = norm * F.binary_cross_entropy_with_logits(preds, labels, pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 / n_nodes * torch.mean(torch.sum(\n",
    "        1 + 2 * logvar - mu.pow(2) - logvar.exp().pow(2), 1))\n",
    "    return cost + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9bcb4490-0d68-461e-b6da-3c2cc97a2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(Module):\n",
    "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout, act=torch.sigmoid):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.dropout(z, self.dropout, training=self.training)\n",
    "        adj = self.act(torch.mm(z, z.t()))\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3fb9cacf-da0f-4a71-9aec-e80cc16e94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModelVAE(Module):\n",
    "    def __init__(self, input_feat_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "        super(GCNModelVAE, self).__init__()\n",
    "        self.gc1 = GCNLayer(input_feat_dim, hidden_dim1, dropout, act=F.relu)\n",
    "        self.gc2 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.gc3 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.dc = InnerProductDecoder(dropout, act=lambda x: x)\n",
    "\n",
    "    def encode(self, x, adj):\n",
    "        hidden1 = self.gc1(x, adj)\n",
    "        return self.gc2(hidden1, adj), self.gc3(hidden1, adj)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        mu, logvar = self.encode(x, adj)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.dc(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "db0813be-93a0-4e77-b7fb-5ea27e912094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6cbfd257-6523-4abf-ad8c-f1f7e07ca0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    \"\"\"\n",
    "    Function to build test set with 10% positive links\n",
    "    \"\"\"\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "    \n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "    \n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "    \n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "    \n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "def get_roc_score(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6de504c0-af1f-4ad4-9fa2-11fd4888e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed' : 42, \n",
    "    'epochs' : 200,\n",
    "    'hidden1' : 64,\n",
    "    'hidden2' : 32,\n",
    "    'lr' : 0.05,\n",
    "    'dropout' : 0.,\n",
    "    'dataset_str' : 'Cora'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "169431df-0d67-4afe-a269-7c61dfabebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    print(\"Using {} dataset\".format(args['dataset_str']))\n",
    "    adj, features = load_dataset(\n",
    "        osp.join('data', 'Planetoid'),\n",
    "        Planetoid,\n",
    "        args['dataset_str']\n",
    "    )\n",
    "    # adj, features = load_dataset(\n",
    "    #     osp.join('data', 'KarateClub'),\n",
    "    #     KarateClub,\n",
    "    #     'KarateClub'\n",
    "    # )\n",
    "    n_nodes, feat_dim = tuple(features.shape)\n",
    "\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "    \n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "    adj = adj_train\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    # adj_label = sparse_to_tuple(adj_label)\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    \n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    \n",
    "    model = GCNModelVAE(feat_dim, args['hidden1'], args['hidden2'], args['dropout'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    \n",
    "    hidden_emb = None\n",
    "    # print(torch.unsqueeze(features, 0).shape, torch.unsqueeze(adj_norm, 0).shape)\n",
    "    for epoch in range(args['epochs']):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        recovered, mu, logvar = model(features, adj_norm)\n",
    "        loss = loss_function(preds=recovered, labels=adj_label,\n",
    "                             mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                             norm=norm, pos_weight=pos_weight)\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        hidden_emb = mu.data.numpy()\n",
    "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "              \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "              \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "              )\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "021b753b-b877-4103-b128-23163d7ab71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cora dataset\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0001 train_loss= 2.31319 val_ap= 0.74717 time= 0.17375\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0002 train_loss= 1.97769 val_ap= 0.71301 time= 0.10004\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0003 train_loss= 1.46324 val_ap= 0.71035 time= 0.09716\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0004 train_loss= 1.00397 val_ap= 0.71371 time= 0.09928\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0005 train_loss= 0.76198 val_ap= 0.71753 time= 0.09559\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0006 train_loss= 0.76545 val_ap= 0.70523 time= 0.08602\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0007 train_loss= 0.78406 val_ap= 0.70346 time= 0.08942\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0008 train_loss= 0.76408 val_ap= 0.71239 time= 0.09347\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0009 train_loss= 0.83149 val_ap= 0.71608 time= 0.09229\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0010 train_loss= 0.77901 val_ap= 0.72547 time= 0.09052\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0011 train_loss= 0.80099 val_ap= 0.72192 time= 0.08794\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0012 train_loss= 0.79040 val_ap= 0.72098 time= 0.08891\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0013 train_loss= 0.79413 val_ap= 0.71791 time= 0.08815\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0014 train_loss= 0.80148 val_ap= 0.71864 time= 0.08512\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0015 train_loss= 0.79097 val_ap= 0.72718 time= 0.08426\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0016 train_loss= 0.78973 val_ap= 0.73695 time= 0.08927\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0017 train_loss= 0.79244 val_ap= 0.74043 time= 0.08586\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0018 train_loss= 0.78579 val_ap= 0.75103 time= 0.08639\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0019 train_loss= 0.78230 val_ap= 0.75838 time= 0.09477\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0020 train_loss= 0.78060 val_ap= 0.76175 time= 0.09419\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0021 train_loss= 0.77768 val_ap= 0.76792 time= 0.08884\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0022 train_loss= 0.77123 val_ap= 0.78431 time= 0.08888\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0023 train_loss= 0.76427 val_ap= 0.79796 time= 0.08557\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0024 train_loss= 0.75341 val_ap= 0.80176 time= 0.09450\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0025 train_loss= 0.73735 val_ap= 0.79852 time= 0.09442\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0026 train_loss= 0.71697 val_ap= 0.79138 time= 0.08956\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0027 train_loss= 0.69748 val_ap= 0.78291 time= 0.09440\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0028 train_loss= 0.68255 val_ap= 0.77282 time= 0.10031\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0029 train_loss= 0.67658 val_ap= 0.77001 time= 0.09333\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0030 train_loss= 0.66866 val_ap= 0.77324 time= 0.09397\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0031 train_loss= 0.65137 val_ap= 0.78161 time= 0.09589\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0032 train_loss= 0.62792 val_ap= 0.80118 time= 0.09153\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0033 train_loss= 0.60370 val_ap= 0.81807 time= 0.09480\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0034 train_loss= 0.58574 val_ap= 0.83177 time= 0.09835\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0035 train_loss= 0.57567 val_ap= 0.83586 time= 0.09611\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0036 train_loss= 0.57109 val_ap= 0.83388 time= 0.09890\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0037 train_loss= 0.56638 val_ap= 0.83640 time= 0.10005\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0038 train_loss= 0.55751 val_ap= 0.84776 time= 0.10153\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0039 train_loss= 0.54677 val_ap= 0.86233 time= 0.10571\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0040 train_loss= 0.53408 val_ap= 0.87597 time= 0.10692\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0041 train_loss= 0.52471 val_ap= 0.88844 time= 0.10323\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0042 train_loss= 0.51691 val_ap= 0.89857 time= 0.10340\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0043 train_loss= 0.51171 val_ap= 0.90053 time= 0.10637\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0044 train_loss= 0.51040 val_ap= 0.89917 time= 0.09916\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0045 train_loss= 0.50843 val_ap= 0.89968 time= 0.09842\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0046 train_loss= 0.50790 val_ap= 0.90143 time= 0.10876\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0047 train_loss= 0.50251 val_ap= 0.90286 time= 0.10163\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0048 train_loss= 0.49938 val_ap= 0.90449 time= 0.09617\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0049 train_loss= 0.49654 val_ap= 0.90919 time= 0.09441\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0050 train_loss= 0.49111 val_ap= 0.90989 time= 0.09950\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0051 train_loss= 0.48817 val_ap= 0.91276 time= 0.09523\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0052 train_loss= 0.48633 val_ap= 0.91586 time= 0.09455\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0053 train_loss= 0.48197 val_ap= 0.91634 time= 0.09713\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0054 train_loss= 0.47853 val_ap= 0.91088 time= 0.10400\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0055 train_loss= 0.47695 val_ap= 0.90779 time= 0.09682\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0056 train_loss= 0.47481 val_ap= 0.90921 time= 0.09988\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0057 train_loss= 0.47198 val_ap= 0.90726 time= 0.09902\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0058 train_loss= 0.47081 val_ap= 0.90573 time= 0.09938\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0059 train_loss= 0.46960 val_ap= 0.90548 time= 0.09707\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0060 train_loss= 0.46800 val_ap= 0.90489 time= 0.15298\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0061 train_loss= 0.46710 val_ap= 0.90490 time= 0.09505\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0062 train_loss= 0.46561 val_ap= 0.90823 time= 0.09647\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0063 train_loss= 0.46409 val_ap= 0.91140 time= 0.09930\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0064 train_loss= 0.46318 val_ap= 0.91313 time= 0.10094\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0065 train_loss= 0.46166 val_ap= 0.91536 time= 0.09662\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0066 train_loss= 0.46021 val_ap= 0.91687 time= 0.09696\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0067 train_loss= 0.45940 val_ap= 0.91597 time= 0.09438\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0068 train_loss= 0.45856 val_ap= 0.91789 time= 0.11434\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0069 train_loss= 0.45753 val_ap= 0.91911 time= 0.09620\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0070 train_loss= 0.45664 val_ap= 0.91894 time= 0.10216\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0071 train_loss= 0.45578 val_ap= 0.91722 time= 0.09799\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0072 train_loss= 0.45547 val_ap= 0.91919 time= 0.10129\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0073 train_loss= 0.45444 val_ap= 0.92050 time= 0.09725\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0074 train_loss= 0.45381 val_ap= 0.91873 time= 0.09642\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0075 train_loss= 0.45317 val_ap= 0.91938 time= 0.09625\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0076 train_loss= 0.45201 val_ap= 0.92289 time= 0.09622\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0077 train_loss= 0.45235 val_ap= 0.92382 time= 0.09554\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0078 train_loss= 0.45080 val_ap= 0.92127 time= 0.09564\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0079 train_loss= 0.45050 val_ap= 0.92359 time= 0.09661\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0080 train_loss= 0.45049 val_ap= 0.92431 time= 0.10019\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0081 train_loss= 0.44921 val_ap= 0.92389 time= 0.09660\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0082 train_loss= 0.44923 val_ap= 0.92339 time= 0.09863\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0083 train_loss= 0.44897 val_ap= 0.92439 time= 0.09863\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0084 train_loss= 0.44824 val_ap= 0.92403 time= 0.09659\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0085 train_loss= 0.44816 val_ap= 0.92364 time= 0.10208\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0086 train_loss= 0.44748 val_ap= 0.92428 time= 0.09946\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0087 train_loss= 0.44756 val_ap= 0.92335 time= 0.10055\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0088 train_loss= 0.44650 val_ap= 0.92400 time= 0.09905\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0089 train_loss= 0.44668 val_ap= 0.92426 time= 0.09774\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0090 train_loss= 0.44574 val_ap= 0.92405 time= 0.09701\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0091 train_loss= 0.44563 val_ap= 0.92259 time= 0.09420\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0092 train_loss= 0.44521 val_ap= 0.92374 time= 0.09704\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0093 train_loss= 0.44510 val_ap= 0.92201 time= 0.09585\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0094 train_loss= 0.44473 val_ap= 0.92465 time= 0.09988\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0095 train_loss= 0.44407 val_ap= 0.92359 time= 0.10190\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0096 train_loss= 0.44408 val_ap= 0.92321 time= 0.09702\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0097 train_loss= 0.44357 val_ap= 0.92381 time= 0.09896\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0098 train_loss= 0.44328 val_ap= 0.92439 time= 0.09426\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0099 train_loss= 0.44302 val_ap= 0.92249 time= 0.09782\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0100 train_loss= 0.44254 val_ap= 0.92372 time= 0.09319\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0101 train_loss= 0.44202 val_ap= 0.92416 time= 0.09560\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0102 train_loss= 0.44241 val_ap= 0.92196 time= 0.09884\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0103 train_loss= 0.44162 val_ap= 0.92486 time= 0.09742\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0104 train_loss= 0.44145 val_ap= 0.92475 time= 0.09758\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0105 train_loss= 0.44120 val_ap= 0.92407 time= 0.10286\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0106 train_loss= 0.44087 val_ap= 0.92543 time= 0.09788\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0107 train_loss= 0.44042 val_ap= 0.92612 time= 0.09903\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0108 train_loss= 0.44025 val_ap= 0.92427 time= 0.09380\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0109 train_loss= 0.43951 val_ap= 0.92685 time= 0.09661\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0110 train_loss= 0.43947 val_ap= 0.92851 time= 0.09955\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0111 train_loss= 0.43939 val_ap= 0.92639 time= 0.09843\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0112 train_loss= 0.43873 val_ap= 0.92758 time= 0.09591\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0113 train_loss= 0.43842 val_ap= 0.92901 time= 0.09941\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0114 train_loss= 0.43770 val_ap= 0.92810 time= 0.09403\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0115 train_loss= 0.43744 val_ap= 0.92767 time= 0.09898\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0116 train_loss= 0.43763 val_ap= 0.92902 time= 0.09540\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0117 train_loss= 0.43704 val_ap= 0.92757 time= 0.09381\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0118 train_loss= 0.43708 val_ap= 0.92802 time= 0.09925\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0119 train_loss= 0.43621 val_ap= 0.92916 time= 0.09634\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0120 train_loss= 0.43607 val_ap= 0.92745 time= 0.09617\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0121 train_loss= 0.43600 val_ap= 0.92731 time= 0.09946\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0122 train_loss= 0.43546 val_ap= 0.92827 time= 0.09932\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0123 train_loss= 0.43550 val_ap= 0.92764 time= 0.10556\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0124 train_loss= 0.43524 val_ap= 0.92660 time= 0.10241\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0125 train_loss= 0.43515 val_ap= 0.92815 time= 0.10253\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0126 train_loss= 0.43447 val_ap= 0.92753 time= 0.10401\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0127 train_loss= 0.43478 val_ap= 0.92738 time= 0.10958\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0128 train_loss= 0.43433 val_ap= 0.92794 time= 0.10458\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0129 train_loss= 0.43396 val_ap= 0.92754 time= 0.09972\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0130 train_loss= 0.43423 val_ap= 0.92728 time= 0.09681\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0131 train_loss= 0.43338 val_ap= 0.92767 time= 0.09610\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0132 train_loss= 0.43350 val_ap= 0.92771 time= 0.09595\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0133 train_loss= 0.43359 val_ap= 0.92822 time= 0.09493\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0134 train_loss= 0.43280 val_ap= 0.92711 time= 0.09996\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0135 train_loss= 0.43288 val_ap= 0.92763 time= 0.09552\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0136 train_loss= 0.43248 val_ap= 0.92773 time= 0.09718\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0137 train_loss= 0.43234 val_ap= 0.92717 time= 0.09694\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0138 train_loss= 0.43230 val_ap= 0.92724 time= 0.09306\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0139 train_loss= 0.43206 val_ap= 0.92741 time= 0.10032\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0140 train_loss= 0.43198 val_ap= 0.92699 time= 0.09490\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0141 train_loss= 0.43183 val_ap= 0.92744 time= 0.09251\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0142 train_loss= 0.43135 val_ap= 0.92760 time= 0.09945\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0143 train_loss= 0.43157 val_ap= 0.92801 time= 0.09348\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0144 train_loss= 0.43143 val_ap= 0.92824 time= 0.09824\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0145 train_loss= 0.43139 val_ap= 0.92750 time= 0.10940\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0146 train_loss= 0.43098 val_ap= 0.92864 time= 0.11845\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0147 train_loss= 0.43090 val_ap= 0.92846 time= 0.09785\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0148 train_loss= 0.43117 val_ap= 0.92793 time= 0.09621\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0149 train_loss= 0.43082 val_ap= 0.92880 time= 0.09897\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0150 train_loss= 0.43139 val_ap= 0.92747 time= 0.09572\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0151 train_loss= 0.43092 val_ap= 0.92900 time= 0.09770\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0152 train_loss= 0.43046 val_ap= 0.92754 time= 0.10242\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0153 train_loss= 0.43009 val_ap= 0.92801 time= 0.10751\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0154 train_loss= 0.43043 val_ap= 0.92864 time= 0.10547\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0155 train_loss= 0.43013 val_ap= 0.92783 time= 0.10782\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0156 train_loss= 0.43033 val_ap= 0.92839 time= 0.09729\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0157 train_loss= 0.43025 val_ap= 0.92879 time= 0.10200\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0158 train_loss= 0.42996 val_ap= 0.92727 time= 0.11796\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0159 train_loss= 0.42993 val_ap= 0.92846 time= 0.10083\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0160 train_loss= 0.42993 val_ap= 0.92883 time= 0.10674\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0161 train_loss= 0.42933 val_ap= 0.92787 time= 0.10381\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0162 train_loss= 0.43000 val_ap= 0.92800 time= 0.09897\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0163 train_loss= 0.42954 val_ap= 0.92737 time= 0.09726\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0164 train_loss= 0.42963 val_ap= 0.92877 time= 0.09559\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0165 train_loss= 0.42861 val_ap= 0.92771 time= 0.09598\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0166 train_loss= 0.42933 val_ap= 0.92723 time= 0.09900\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0167 train_loss= 0.42924 val_ap= 0.93014 time= 0.09723\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0168 train_loss= 0.42851 val_ap= 0.92788 time= 0.10213\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0169 train_loss= 0.42877 val_ap= 0.92741 time= 0.10049\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0170 train_loss= 0.42882 val_ap= 0.93005 time= 0.10315\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0171 train_loss= 0.42817 val_ap= 0.92926 time= 0.10408\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0172 train_loss= 0.42817 val_ap= 0.92821 time= 0.10009\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0173 train_loss= 0.42838 val_ap= 0.93059 time= 0.10463\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0174 train_loss= 0.42792 val_ap= 0.93033 time= 0.10084\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0175 train_loss= 0.42833 val_ap= 0.92814 time= 0.10322\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0176 train_loss= 0.42812 val_ap= 0.93157 time= 0.09722\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0177 train_loss= 0.42738 val_ap= 0.93016 time= 0.10379\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0178 train_loss= 0.42800 val_ap= 0.92866 time= 0.09447\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0179 train_loss= 0.42784 val_ap= 0.93160 time= 0.09588\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0180 train_loss= 0.42746 val_ap= 0.93167 time= 0.09723\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0181 train_loss= 0.42763 val_ap= 0.92916 time= 0.09780\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0182 train_loss= 0.42711 val_ap= 0.93102 time= 0.10298\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0183 train_loss= 0.42729 val_ap= 0.93222 time= 0.09884\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0184 train_loss= 0.42745 val_ap= 0.93048 time= 0.09550\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0185 train_loss= 0.42671 val_ap= 0.93131 time= 0.09581\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0186 train_loss= 0.42724 val_ap= 0.93247 time= 0.10129\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0187 train_loss= 0.42679 val_ap= 0.93032 time= 0.09693\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0188 train_loss= 0.42663 val_ap= 0.93117 time= 0.09867\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0189 train_loss= 0.42655 val_ap= 0.93260 time= 0.09769\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0190 train_loss= 0.42679 val_ap= 0.93148 time= 0.11102\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0191 train_loss= 0.42629 val_ap= 0.93136 time= 0.10747\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0192 train_loss= 0.42656 val_ap= 0.93191 time= 0.10133\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0193 train_loss= 0.42600 val_ap= 0.93170 time= 0.10392\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0194 train_loss= 0.42631 val_ap= 0.93155 time= 0.11736\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0195 train_loss= 0.42609 val_ap= 0.93171 time= 0.12526\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0196 train_loss= 0.42590 val_ap= 0.93114 time= 0.09924\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0197 train_loss= 0.42599 val_ap= 0.93152 time= 0.09563\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0198 train_loss= 0.42584 val_ap= 0.93206 time= 0.09509\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0199 train_loss= 0.42560 val_ap= 0.93191 time= 0.09944\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0200 train_loss= 0.42588 val_ap= 0.93188 time= 0.09706\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9021384155057628\n",
      "Test AP score: 0.9129125372465432\n"
     ]
    }
   ],
   "source": [
    "model = build_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25271c8f-93d3-41b2-b20c-402ea69c95db",
   "metadata": {},
   "source": [
    "## Exp 2: VGAE using ChebConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "07a33c06-aaf4-45a6-a777-973cef995732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn.models import InnerProductDecoder, VGAE\n",
    "from torch_geometric.nn.conv import GCNConv, ChebConv, SAGEConv, GraphConv, ResGatedGraphConv, GATConv, GATv2Conv\n",
    "from torch_geometric.utils import negative_sampling, remove_self_loops, add_self_loops\n",
    "\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "81dd9140-ea6b-4ad1-b555-29fd1debc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.gcn_shared = GATv2Conv(in_channels, hidden_channels)\n",
    "        self.gcn_mu = GATv2Conv(hidden_channels, out_channels)\n",
    "        self.gcn_logvar = GATv2Conv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gcn_shared(x, edge_index))\n",
    "        mu = self.gcn_mu(x, edge_index)\n",
    "        logvar = self.gcn_logvar(x, edge_index)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class DeepVGAE(VGAE):\n",
    "    def __init__(self, args):\n",
    "        super(DeepVGAE, self).__init__(encoder=GCNEncoder(args[\"enc_in_channels\"],\n",
    "                                                          args[\"enc_hidden_channels\"],\n",
    "                                                          args[\"enc_out_channels\"]),\n",
    "                                       decoder=InnerProductDecoder())\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        adj_pred = self.decoder.forward_all(z)\n",
    "        return adj_pred\n",
    "\n",
    "    def loss(self, x, pos_edge_index, all_edge_index):\n",
    "        z = self.encode(x, pos_edge_index)\n",
    "\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(z, pos_edge_index, sigmoid=True) + 1e-15).mean()\n",
    "\n",
    "        # Do not include self-loops in negative samples\n",
    "        all_edge_index_tmp, _ = remove_self_loops(all_edge_index)\n",
    "        all_edge_index_tmp, _ = add_self_loops(all_edge_index_tmp)\n",
    "\n",
    "        neg_edge_index = negative_sampling(all_edge_index_tmp, z.size(0), pos_edge_index.size(1))\n",
    "        neg_loss = -torch.log(1 - self.decoder(z, neg_edge_index, sigmoid=True) + 1e-15).mean()\n",
    "\n",
    "        kl_loss = 1 / x.size(0) * self.kl_loss()\n",
    "\n",
    "        return pos_loss + neg_loss + kl_loss\n",
    "\n",
    "    def single_test(self, x, train_pos_edge_index, test_pos_edge_index, test_neg_edge_index):\n",
    "        with torch.no_grad():\n",
    "            z = self.encode(x, train_pos_edge_index)\n",
    "        roc_auc_score, average_precision_score = self.test(z, test_pos_edge_index, test_neg_edge_index)\n",
    "        return roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f48aeafc-2fd3-4b93-95c9-0ec8edf401d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset\": \"Cora\",\n",
    "    \"enc_in_channels\": 1433,\n",
    "    \"enc_hidden_channels\": 128,\n",
    "    \"enc_out_channels\": 64,\n",
    "    \"lr\": 0.007,\n",
    "    \"epoch\": 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d14bf9be-a016-4ae1-808f-df778527b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2c8275e5-ac88-40be-9054-13649b313cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashutosh1919/miniforge3/envs/persona2vec/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "model = DeepVGAE(args).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "dataset = Planetoid(os.path.join('data', 'Planetoid'), args[\"dataset\"], transform=T.NormalizeFeatures())\n",
    "data = dataset[0].to(device)\n",
    "all_edge_index = data.edge_index\n",
    "data = train_test_split_edges(data, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8af783a0-1b22-4af1-8ac2-f384105d5511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 7.020147323608398 ROC_AUC: 0.512834453730075 Precision: 0.5619028995951968\n",
      "Epoch 1 - Loss: 6.353734016418457 ROC_AUC: 0.5404477026165795 Precision: 0.5688020983381454\n",
      "Epoch 2 - Loss: 5.948556423187256 ROC_AUC: 0.5545135725833457 Precision: 0.5685748986836247\n",
      "Epoch 3 - Loss: 5.289628505706787 ROC_AUC: 0.5640012386175012 Precision: 0.5644605540367131\n",
      "Epoch 4 - Loss: 4.593630790710449 ROC_AUC: 0.5601827680940773 Precision: 0.5574427438826639\n",
      "Epoch 5 - Loss: 3.9499638080596924 ROC_AUC: 0.5565137238099009 Precision: 0.5543630786466682\n",
      "Epoch 6 - Loss: 3.3577821254730225 ROC_AUC: 0.5451429270979984 Precision: 0.5463557489838422\n",
      "Epoch 7 - Loss: 2.827617883682251 ROC_AUC: 0.5265132557277058 Precision: 0.526604150797414\n",
      "Epoch 8 - Loss: 2.3036341667175293 ROC_AUC: 0.49540379290603426 Precision: 0.5060980026865113\n",
      "Epoch 9 - Loss: 1.9329620599746704 ROC_AUC: 0.4918355663254468 Precision: 0.50902803072839\n",
      "Epoch 10 - Loss: 1.7288002967834473 ROC_AUC: 0.5130792967245048 Precision: 0.5283147199205707\n",
      "Epoch 11 - Loss: 1.6090795993804932 ROC_AUC: 0.5291003100144386 Precision: 0.5477772947947339\n",
      "Epoch 12 - Loss: 1.520408272743225 ROC_AUC: 0.5576947311947978 Precision: 0.5664499835342844\n",
      "Epoch 13 - Loss: 1.4725829362869263 ROC_AUC: 0.598110028120938 Precision: 0.5924293227074475\n",
      "Epoch 14 - Loss: 1.454591989517212 ROC_AUC: 0.6177460762109828 Precision: 0.614510592054904\n",
      "Epoch 15 - Loss: 1.4458328485488892 ROC_AUC: 0.6081737953184579 Precision: 0.6117645320924717\n",
      "Epoch 16 - Loss: 1.4498989582061768 ROC_AUC: 0.599859935404657 Precision: 0.5995709359617338\n",
      "Epoch 17 - Loss: 1.4467835426330566 ROC_AUC: 0.5939008889961077 Precision: 0.5889349649182911\n",
      "Epoch 18 - Loss: 1.4408748149871826 ROC_AUC: 0.5726445563841011 Precision: 0.5624673756839247\n",
      "Epoch 19 - Loss: 1.441131353378296 ROC_AUC: 0.5498039455728426 Precision: 0.538539747793007\n",
      "Epoch 20 - Loss: 1.4492597579956055 ROC_AUC: 0.5271253632137802 Precision: 0.5223521980593726\n",
      "Epoch 21 - Loss: 1.4510645866394043 ROC_AUC: 0.5188601118356384 Precision: 0.5236178925214565\n",
      "Epoch 22 - Loss: 1.4494574069976807 ROC_AUC: 0.526342225694832 Precision: 0.5494057713052114\n",
      "Epoch 23 - Loss: 1.4486329555511475 ROC_AUC: 0.5424118475204246 Precision: 0.577753118519672\n",
      "Epoch 24 - Loss: 1.4489976167678833 ROC_AUC: 0.5764486243784408 Precision: 0.597953206948971\n",
      "Epoch 25 - Loss: 1.4515161514282227 ROC_AUC: 0.6087336936366026 Precision: 0.6158612174294396\n",
      "Epoch 26 - Loss: 1.4530377388000488 ROC_AUC: 0.628481361327049 Precision: 0.636522940847681\n",
      "Epoch 27 - Loss: 1.4543585777282715 ROC_AUC: 0.6315256959122022 Precision: 0.6408295380128249\n",
      "Epoch 28 - Loss: 1.4558974504470825 ROC_AUC: 0.6155568917901983 Precision: 0.6304468284677749\n",
      "Epoch 29 - Loss: 1.455597162246704 ROC_AUC: 0.5961224791073312 Precision: 0.6192027439190995\n",
      "Epoch 30 - Loss: 1.4552781581878662 ROC_AUC: 0.5835184658426019 Precision: 0.6107901386297686\n",
      "Epoch 31 - Loss: 1.4551414251327515 ROC_AUC: 0.5841125701673213 Precision: 0.6063258515214338\n",
      "Epoch 32 - Loss: 1.4557290077209473 ROC_AUC: 0.6021733416387918 Precision: 0.6116131323600038\n",
      "Epoch 33 - Loss: 1.4572405815124512 ROC_AUC: 0.6361579093288781 Precision: 0.6320832487482895\n",
      "Epoch 34 - Loss: 1.4579075574874878 ROC_AUC: 0.6458040031829588 Precision: 0.6352200735962874\n",
      "Epoch 35 - Loss: 1.4570165872573853 ROC_AUC: 0.6344512096324115 Precision: 0.627784183047803\n",
      "Epoch 36 - Loss: 1.4576281309127808 ROC_AUC: 0.6371030753000226 Precision: 0.6420323599659586\n",
      "Epoch 37 - Loss: 1.457647442817688 ROC_AUC: 0.6579399342524547 Precision: 0.669278886140722\n",
      "Epoch 38 - Loss: 1.4574843645095825 ROC_AUC: 0.6869682316214727 Precision: 0.7039038196404522\n",
      "Epoch 39 - Loss: 1.4572092294692993 ROC_AUC: 0.6772537257542425 Precision: 0.6932218116228999\n",
      "Epoch 40 - Loss: 1.4572821855545044 ROC_AUC: 0.6508718931044291 Precision: 0.6607612823554598\n",
      "Epoch 41 - Loss: 1.45742666721344 ROC_AUC: 0.6261733560413208 Precision: 0.6420669523100884\n",
      "Epoch 42 - Loss: 1.4571112394332886 ROC_AUC: 0.6151788254017405 Precision: 0.6358739447676643\n",
      "Epoch 43 - Loss: 1.4568144083023071 ROC_AUC: 0.6125827695343302 Precision: 0.6343985434826231\n",
      "Epoch 44 - Loss: 1.4560530185699463 ROC_AUC: 0.609808482369504 Precision: 0.6300715266852269\n",
      "Epoch 45 - Loss: 1.4550833702087402 ROC_AUC: 0.603818830586651 Precision: 0.6245433215289926\n",
      "Epoch 46 - Loss: 1.454602599143982 ROC_AUC: 0.6007816972660399 Precision: 0.620594201955459\n",
      "Epoch 47 - Loss: 1.4531582593917847 ROC_AUC: 0.5956489959636913 Precision: 0.6168263844025871\n",
      "Epoch 48 - Loss: 1.4514174461364746 ROC_AUC: 0.592512845255627 Precision: 0.6143237147073979\n",
      "Epoch 49 - Loss: 1.4489835500717163 ROC_AUC: 0.5865429969502645 Precision: 0.6099974789141857\n",
      "Epoch 50 - Loss: 1.445370078086853 ROC_AUC: 0.5802598936373227 Precision: 0.6059610847131309\n",
      "Epoch 51 - Loss: 1.4413503408432007 ROC_AUC: 0.5785928008958373 Precision: 0.6029960051000693\n",
      "Epoch 52 - Loss: 1.4362214803695679 ROC_AUC: 0.5911698094185339 Precision: 0.6074544732561317\n",
      "Epoch 53 - Loss: 1.4284958839416504 ROC_AUC: 0.6174958322681463 Precision: 0.6175704738492547\n",
      "Epoch 54 - Loss: 1.4171615839004517 ROC_AUC: 0.6372597028038125 Precision: 0.627607938181547\n",
      "Epoch 55 - Loss: 1.4025310277938843 ROC_AUC: 0.6561648225428385 Precision: 0.6408044801447857\n",
      "Epoch 56 - Loss: 1.3822513818740845 ROC_AUC: 0.6552358594169136 Precision: 0.6447712111394363\n",
      "Epoch 57 - Loss: 1.354556679725647 ROC_AUC: 0.6394650900698162 Precision: 0.6413979184431782\n",
      "Epoch 58 - Loss: 1.324479103088379 ROC_AUC: 0.6391878413849472 Precision: 0.638968125606406\n",
      "Epoch 59 - Loss: 1.3108186721801758 ROC_AUC: 0.6244936610868868 Precision: 0.6318066001865027\n",
      "Epoch 60 - Loss: 1.3287550210952759 ROC_AUC: 0.6434113830388616 Precision: 0.6417194596371782\n",
      "Epoch 61 - Loss: 1.395756483078003 ROC_AUC: 0.6322170173082393 Precision: 0.638293904088689\n",
      "Epoch 62 - Loss: 1.3166533708572388 ROC_AUC: 0.6302510720882587 Precision: 0.6378400849393095\n",
      "Epoch 63 - Loss: 1.3307956457138062 ROC_AUC: 0.6531907002869703 Precision: 0.6486789185461281\n",
      "Epoch 64 - Loss: 1.2984224557876587 ROC_AUC: 0.6604315717840052 Precision: 0.6515994456111629\n",
      "Epoch 65 - Loss: 1.2971460819244385 ROC_AUC: 0.652056501121597 Precision: 0.6547129768452509\n",
      "Epoch 66 - Loss: 1.2830103635787964 ROC_AUC: 0.6507746760331115 Precision: 0.6561404789873011\n",
      "Epoch 67 - Loss: 1.2890442609786987 ROC_AUC: 0.6690190797504041 Precision: 0.6635041606211242\n",
      "Epoch 68 - Loss: 1.2732020616531372 ROC_AUC: 0.6791332557997185 Precision: 0.663609441942601\n",
      "Epoch 69 - Loss: 1.2877856492996216 ROC_AUC: 0.6784599375650364 Precision: 0.6674610620545949\n",
      "Epoch 70 - Loss: 1.2612828016281128 ROC_AUC: 0.6698652283340956 Precision: 0.6696295506595504\n",
      "Epoch 71 - Loss: 1.2696009874343872 ROC_AUC: 0.6708878078990671 Precision: 0.67126893425241\n",
      "Epoch 72 - Loss: 1.2598978281021118 ROC_AUC: 0.6808615592898113 Precision: 0.6719345796518286\n",
      "Epoch 73 - Loss: 1.2500712871551514 ROC_AUC: 0.6840373169528569 Precision: 0.6737781648291977\n",
      "Epoch 74 - Loss: 1.242770791053772 ROC_AUC: 0.6777038047881208 Precision: 0.6773310601453155\n",
      "Epoch 75 - Loss: 1.234971284866333 ROC_AUC: 0.6782402989965037 Precision: 0.6790338522787808\n",
      "Epoch 76 - Loss: 1.2269015312194824 ROC_AUC: 0.6859024444692488 Precision: 0.6786496594244878\n",
      "Epoch 77 - Loss: 1.227066993713379 ROC_AUC: 0.6886425256275002 Precision: 0.6807004124135843\n",
      "Epoch 78 - Loss: 1.220427393913269 ROC_AUC: 0.6876235466947997 Precision: 0.6852544568489627\n",
      "Epoch 79 - Loss: 1.220636248588562 ROC_AUC: 0.6921459408272091 Precision: 0.6882712840894298\n",
      "Epoch 80 - Loss: 1.205957055091858 ROC_AUC: 0.7022421137151683 Precision: 0.6900051457111441\n",
      "Epoch 81 - Loss: 1.2060047388076782 ROC_AUC: 0.7084316005890634 Precision: 0.694456484540289\n",
      "Epoch 82 - Loss: 1.1989011764526367 ROC_AUC: 0.709843048439306 Precision: 0.7041749413656826\n",
      "Epoch 83 - Loss: 1.1926034688949585 ROC_AUC: 0.719003056936798 Precision: 0.7104112875963667\n",
      "Epoch 84 - Loss: 1.1758862733840942 ROC_AUC: 0.7281774679633745 Precision: 0.7134875268232104\n",
      "Epoch 85 - Loss: 1.174322247505188 ROC_AUC: 0.7316520781049152 Precision: 0.7204538999574183\n",
      "Epoch 86 - Loss: 1.175733208656311 ROC_AUC: 0.7343741561018113 Precision: 0.7267849467700629\n",
      "Epoch 87 - Loss: 1.1543219089508057 ROC_AUC: 0.7396166766884266 Precision: 0.7303946591266948\n",
      "Epoch 88 - Loss: 1.146719217300415 ROC_AUC: 0.7450104238304247 Precision: 0.7324702591516921\n",
      "Epoch 89 - Loss: 1.1504175662994385 ROC_AUC: 0.7452732699862096 Precision: 0.7398284046136187\n",
      "Epoch 90 - Loss: 1.14017653465271 ROC_AUC: 0.7477649075177601 Precision: 0.7435003988540934\n",
      "Epoch 91 - Loss: 1.137300729751587 ROC_AUC: 0.7527913901681135 Precision: 0.7449071744244264\n",
      "Epoch 92 - Loss: 1.1171011924743652 ROC_AUC: 0.7585848074921956 Precision: 0.7512210014674716\n",
      "Epoch 93 - Loss: 1.111592173576355 ROC_AUC: 0.7624230814931102 Precision: 0.7567467387648142\n",
      "Epoch 94 - Loss: 1.1107969284057617 ROC_AUC: 0.7691454619431171 Precision: 0.7606315586276136\n",
      "Epoch 95 - Loss: 1.110898494720459 ROC_AUC: 0.7750793039257694 Precision: 0.7630680187807736\n",
      "Epoch 96 - Loss: 1.103844165802002 ROC_AUC: 0.7816324546590381 Precision: 0.7696735390092869\n",
      "Epoch 97 - Loss: 1.1040818691253662 ROC_AUC: 0.7846065769149062 Precision: 0.7717545539198516\n",
      "Epoch 98 - Loss: 1.110552191734314 ROC_AUC: 0.7946757450608328 Precision: 0.7774114773660196\n",
      "Epoch 99 - Loss: 1.0964022874832153 ROC_AUC: 0.7976300638392102 Precision: 0.7798049839714842\n",
      "Epoch 100 - Loss: 1.0907323360443115 ROC_AUC: 0.7994411818715367 Precision: 0.7805003703567798\n",
      "Epoch 101 - Loss: 1.079636573791504 ROC_AUC: 0.8107939754220841 Precision: 0.7930586037362537\n",
      "Epoch 102 - Loss: 1.0629068613052368 ROC_AUC: 0.8134944496253542 Precision: 0.7954768638802001\n",
      "Epoch 103 - Loss: 1.0846858024597168 ROC_AUC: 0.8106247457053458 Precision: 0.7883916612142896\n",
      "Epoch 104 - Loss: 1.0690568685531616 ROC_AUC: 0.8199863896100155 Precision: 0.8006928199597722\n",
      "Epoch 105 - Loss: 1.0539580583572388 ROC_AUC: 0.8255349639396676 Precision: 0.809077531109389\n",
      "Epoch 106 - Loss: 1.0429574251174927 ROC_AUC: 0.8228452916332109 Precision: 0.8018263287280942\n",
      "Epoch 107 - Loss: 1.039207935333252 ROC_AUC: 0.825146095654397 Precision: 0.8022972904606707\n",
      "Epoch 108 - Loss: 1.0432233810424805 ROC_AUC: 0.8308891041266847 Precision: 0.8151682827703116\n",
      "Epoch 109 - Loss: 1.043988823890686 ROC_AUC: 0.827979793251695 Precision: 0.8118224687303132\n",
      "Epoch 110 - Loss: 1.0469928979873657 ROC_AUC: 0.8279509881935269 Precision: 0.8072195922853078\n",
      "Epoch 111 - Loss: 1.0412886142730713 ROC_AUC: 0.8324121715773289 Precision: 0.8151419344694133\n",
      "Epoch 112 - Loss: 1.0353705883026123 ROC_AUC: 0.8340612611574593 Precision: 0.8179467610472639\n",
      "Epoch 113 - Loss: 1.0430306196212769 ROC_AUC: 0.83388843080845 Precision: 0.8150764966403827\n",
      "Epoch 114 - Loss: 1.0315029621124268 ROC_AUC: 0.8377339060739065 Precision: 0.8193753170251878\n",
      "Epoch 115 - Loss: 1.035517930984497 ROC_AUC: 0.8422310957804191 Precision: 0.8269667974218358\n",
      "Epoch 116 - Loss: 1.0253146886825562 ROC_AUC: 0.8411022975634521 Precision: 0.8245152447048767\n",
      "Epoch 117 - Loss: 1.0303386449813843 ROC_AUC: 0.841827824966064 Precision: 0.8227164408763896\n",
      "Epoch 118 - Loss: 1.0212513208389282 ROC_AUC: 0.8472287733726042 Precision: 0.8309641176903391\n",
      "Epoch 119 - Loss: 1.0136897563934326 ROC_AUC: 0.8491587122698746 Precision: 0.8348420457672275\n",
      "Epoch 120 - Loss: 1.014466404914856 ROC_AUC: 0.8471315563012866 Precision: 0.830000442260012\n",
      "Epoch 121 - Loss: 1.0030823945999146 ROC_AUC: 0.8511102549607711 Precision: 0.8341147298789382\n",
      "Epoch 122 - Loss: 1.0046132802963257 ROC_AUC: 0.853112206503462 Precision: 0.8371843749170341\n",
      "Epoch 123 - Loss: 1.014670491218567 ROC_AUC: 0.8513586985874719 Precision: 0.8360219947019181\n",
      "Epoch 124 - Loss: 1.0052422285079956 ROC_AUC: 0.8551825700593025 Precision: 0.8396489879729956\n",
      "Epoch 125 - Loss: 0.9986769556999207 ROC_AUC: 0.8581350885215444 Precision: 0.8448994780320628\n",
      "Epoch 126 - Loss: 1.001349687576294 ROC_AUC: 0.8576886101199371 Precision: 0.8460058329702966\n",
      "Epoch 127 - Loss: 0.9874102473258972 ROC_AUC: 0.8578110316171519 Precision: 0.8479220190678942\n",
      "Epoch 128 - Loss: 0.9989285469055176 ROC_AUC: 0.8621461928714683 Precision: 0.8525471435436904\n",
      "Epoch 129 - Loss: 0.9915681481361389 ROC_AUC: 0.8619337555674776 Precision: 0.8523006569889103\n",
      "Epoch 130 - Loss: 0.9835096001625061 ROC_AUC: 0.861048000028805 Precision: 0.8522053783615383\n",
      "Epoch 131 - Loss: 0.9925838112831116 ROC_AUC: 0.8665821718293732 Precision: 0.8603847260291215\n",
      "Epoch 132 - Loss: 0.9924542903900146 ROC_AUC: 0.8669602382178312 Precision: 0.8605517203778307\n",
      "Epoch 133 - Loss: 0.9759070873260498 ROC_AUC: 0.8637700780257013 Precision: 0.8548200152959939\n",
      "Epoch 134 - Loss: 0.9772047996520996 ROC_AUC: 0.8690594068318397 Precision: 0.861579861558766\n",
      "Epoch 135 - Loss: 0.9798156023025513 ROC_AUC: 0.8715582456279323 Precision: 0.8648511536384906\n",
      "Epoch 136 - Loss: 0.9811692833900452 ROC_AUC: 0.8675255374843822 Precision: 0.8587938851348542\n",
      "Epoch 137 - Loss: 0.9769978523254395 ROC_AUC: 0.8676479589815971 Precision: 0.8580471058108732\n",
      "Epoch 138 - Loss: 0.9735531806945801 ROC_AUC: 0.8746547893810153 Precision: 0.8656022326031008\n",
      "Epoch 139 - Loss: 0.9709956049919128 ROC_AUC: 0.8728328694518758 Precision: 0.8633002620584761\n",
      "Epoch 140 - Loss: 0.9572302103042603 ROC_AUC: 0.8701179927195215 Precision: 0.8585236318662295\n",
      "Epoch 141 - Loss: 0.9740375876426697 ROC_AUC: 0.8744855596642771 Precision: 0.8643617848018259\n",
      "Epoch 142 - Loss: 0.9726957082748413 ROC_AUC: 0.8739922730431464 Precision: 0.8644664326993083\n",
      "Epoch 143 - Loss: 0.9638118743896484 ROC_AUC: 0.8761094448185102 Precision: 0.8653955762429006\n",
      "Epoch 144 - Loss: 0.964713454246521 ROC_AUC: 0.8757025733718841 Precision: 0.8642241557476384\n",
      "Epoch 145 - Loss: 0.9738016724586487 ROC_AUC: 0.875554947448772 Precision: 0.8646967592828507\n",
      "Epoch 146 - Loss: 0.9682390689849854 ROC_AUC: 0.8758429980304541 Precision: 0.8651974083474108\n",
      "Epoch 147 - Loss: 0.9543951749801636 ROC_AUC: 0.8756665670491739 Precision: 0.8647909234447917\n",
      "Epoch 148 - Loss: 0.9684004187583923 ROC_AUC: 0.8812187420110972 Precision: 0.8711216562909259\n",
      "Epoch 149 - Loss: 0.947888195514679 ROC_AUC: 0.8767575586272949 Precision: 0.8665963963838823\n",
      "Epoch 150 - Loss: 0.9590872526168823 ROC_AUC: 0.8767791624209211 Precision: 0.8658038456574318\n",
      "Epoch 151 - Loss: 0.9615235328674316 ROC_AUC: 0.8818380507617137 Precision: 0.8723224273077814\n",
      "Epoch 152 - Loss: 0.9587975144386292 ROC_AUC: 0.8819100634071343 Precision: 0.872376605810109\n",
      "Epoch 153 - Loss: 0.9490996599197388 ROC_AUC: 0.8785614753950793 Precision: 0.8688354410183934\n",
      "Epoch 154 - Loss: 0.9620302319526672 ROC_AUC: 0.8832675017733114 Precision: 0.875068454655621\n",
      "Epoch 155 - Loss: 0.9440667629241943 ROC_AUC: 0.8826769980808631 Precision: 0.8734463765118003\n",
      "Epoch 156 - Loss: 0.9453070759773254 ROC_AUC: 0.8803581908983218 Precision: 0.8699823325400473\n",
      "Epoch 157 - Loss: 0.9507485032081604 ROC_AUC: 0.8844125028354978 Precision: 0.87521875444207\n",
      "Epoch 158 - Loss: 0.9579969048500061 ROC_AUC: 0.8850354122183857 Precision: 0.8771069471252472\n",
      "Epoch 159 - Loss: 0.9517964720726013 ROC_AUC: 0.8825725797450031 Precision: 0.8728246878532844\n",
      "Epoch 160 - Loss: 0.9415870308876038 ROC_AUC: 0.8839372193757224 Precision: 0.8741340185751475\n",
      "Epoch 161 - Loss: 0.9413306713104248 ROC_AUC: 0.8866124891530953 Precision: 0.8799446285429892\n",
      "Epoch 162 - Loss: 0.9452611207962036 ROC_AUC: 0.8827094037713022 Precision: 0.8753931129292025\n",
      "Epoch 163 - Loss: 0.9349029660224915 ROC_AUC: 0.8833035080960217 Precision: 0.8752774577028651\n",
      "Epoch 164 - Loss: 0.9415926337242126 ROC_AUC: 0.8854314817681985 Precision: 0.8784207003956275\n",
      "Epoch 165 - Loss: 0.9443718194961548 ROC_AUC: 0.8854206798713854 Precision: 0.8801790585545549\n",
      "Epoch 166 - Loss: 0.9344594478607178 ROC_AUC: 0.8835375491936385 Precision: 0.8772203377538679\n",
      "Epoch 167 - Loss: 0.933593213558197 ROC_AUC: 0.8839912288597878 Precision: 0.8772278763946751\n",
      "Epoch 168 - Loss: 0.9457278251647949 ROC_AUC: 0.8860579917833572 Precision: 0.88168999417082\n",
      "Epoch 169 - Loss: 0.9409067630767822 ROC_AUC: 0.885363069755049 Precision: 0.8825545716948482\n",
      "Epoch 170 - Loss: 0.9421148300170898 ROC_AUC: 0.8833611182123581 Precision: 0.8791095161444281\n",
      "Epoch 171 - Loss: 0.9446802139282227 ROC_AUC: 0.8863784480554785 Precision: 0.8811981663649752\n",
      "Epoch 172 - Loss: 0.932877779006958 ROC_AUC: 0.8855286988395162 Precision: 0.8836123994285161\n",
      "Epoch 173 - Loss: 0.9347670078277588 ROC_AUC: 0.8858239506857405 Precision: 0.883640054563589\n",
      "Epoch 174 - Loss: 0.9327899217605591 ROC_AUC: 0.8866160897853663 Precision: 0.8839642407853655\n",
      "Epoch 175 - Loss: 0.9325382113456726 ROC_AUC: 0.8863424417327682 Precision: 0.8835057296777541\n",
      "Epoch 176 - Loss: 0.9335837364196777 ROC_AUC: 0.8853810729164041 Precision: 0.8850931946966071\n",
      "Epoch 177 - Loss: 0.9315927624702454 ROC_AUC: 0.8865044701849645 Precision: 0.8860397922162111\n",
      "Epoch 178 - Loss: 0.9292961359024048 ROC_AUC: 0.8889853058197019 Precision: 0.8864449237410204\n",
      "Epoch 179 - Loss: 0.9293144941329956 ROC_AUC: 0.8858347525825535 Precision: 0.8840680196045617\n",
      "Epoch 180 - Loss: 0.9252820014953613 ROC_AUC: 0.8854062773423014 Precision: 0.8848757322945695\n",
      "Epoch 181 - Loss: 0.9328856468200684 ROC_AUC: 0.888862884322487 Precision: 0.8879887970540284\n",
      "Epoch 182 - Loss: 0.9353399872779846 ROC_AUC: 0.8889853058197019 Precision: 0.8871675239189284\n",
      "Epoch 183 - Loss: 0.929490864276886 ROC_AUC: 0.8851974406705818 Precision: 0.8855214152209705\n",
      "Epoch 184 - Loss: 0.9312475919723511 ROC_AUC: 0.8869941561738242 Precision: 0.889009995305696\n",
      "Epoch 185 - Loss: 0.9235044121742249 ROC_AUC: 0.8901555113077856 Precision: 0.8911813960720399\n",
      "Epoch 186 - Loss: 0.9209534525871277 ROC_AUC: 0.8880923490164873 Precision: 0.888290243389025\n",
      "Epoch 187 - Loss: 0.9211110472679138 ROC_AUC: 0.8865908853594691 Precision: 0.8888315609031259\n",
      "Epoch 188 - Loss: 0.9249405860900879 ROC_AUC: 0.8886036387989731 Precision: 0.891779322349562\n",
      "Epoch 189 - Loss: 0.9239515662193298 ROC_AUC: 0.8897666430225147 Precision: 0.8909907560259198\n",
      "Epoch 190 - Loss: 0.9275268316268921 ROC_AUC: 0.8874190307818053 Precision: 0.8885572395124718\n",
      "Epoch 191 - Loss: 0.9318920373916626 ROC_AUC: 0.8872714048586932 Precision: 0.8904404674969094\n",
      "Epoch 192 - Loss: 0.9206351041793823 ROC_AUC: 0.8882219717782442 Precision: 0.8929380067277575\n",
      "Epoch 193 - Loss: 0.9382906556129456 ROC_AUC: 0.889932272106982 Precision: 0.891941371943663\n",
      "Epoch 194 - Loss: 0.921956479549408 ROC_AUC: 0.8874694396335997 Precision: 0.8900847107258165\n",
      "Epoch 195 - Loss: 0.9163210988044739 ROC_AUC: 0.886236223080773 Precision: 0.8898832300925521\n",
      "Epoch 196 - Loss: 0.9220656156539917 ROC_AUC: 0.8875774586017305 Precision: 0.8920268584251049\n",
      "Epoch 197 - Loss: 0.931941032409668 ROC_AUC: 0.889806249977496 Precision: 0.8929807354462106\n",
      "Epoch 198 - Loss: 0.9182433485984802 ROC_AUC: 0.8874874427949548 Precision: 0.8905450681599855\n",
      "Epoch 199 - Loss: 0.9133180975914001 ROC_AUC: 0.8859643753443105 Precision: 0.8898852043085478\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args[\"epoch\"]):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.loss(data.x, data.train_pos_edge_index, all_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "        roc_auc, ap = model.single_test(data.x,\n",
    "                                        data.train_pos_edge_index,\n",
    "                                        data.test_pos_edge_index,\n",
    "                                        data.test_neg_edge_index)\n",
    "        print(\"Epoch {} - Loss: {} ROC_AUC: {} Precision: {}\".format(epoch, loss.cpu().item(), roc_auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629a737-030b-4f8a-80a9-7897349bde8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona2vec",
   "language": "python",
   "name": "persona2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
