{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd32f20-f0f2-47c2-9786-51dc1dc0dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.loss\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3131901-cc81-4a9f-96f1-ee7e0e3d45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, dataset_cls, dataset_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device)\n",
    "    ])\n",
    "    obj = dataset_cls(data_dir, dataset_name, transform=transform)\n",
    "    network = to_networkx(obj[0])\n",
    "    return nx.adjacency_matrix(network), obj[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e84707-7e3c-4ef0-a643-43c9c7fb29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features = load_dataset(\n",
    "    osp.join('data', 'Planetoid'),\n",
    "    Planetoid,\n",
    "    'Cora'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e375f5ed-5793-4702-9c62-ae52e27e68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Adj. Matrix: (2708, 2708)\n",
      "Shape of features: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Adj. Matrix: {}\".format(adj.shape))\n",
    "print(\"Shape of features: {}\".format(tuple(features.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fb7e3e-6428-4514-ade2-296ab125de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0., act=F.relu):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input_tensor, adj):\n",
    "        input_tensor = F.dropout(input_tensor, self.dropout, self.training)\n",
    "        support = torch.mm(input_tensor, self.weight)\n",
    "        output_tensor = torch.spmm(adj, support)\n",
    "        output_tensor = self.act(output_tensor)\n",
    "        return output_tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610a3545-32f0-4220-bbb2-42f7cfacdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, labels, mu, logvar, n_nodes, norm, pos_weight):\n",
    "    print(type(pos_weight))\n",
    "    cost = norm * F.binary_cross_entropy_with_logits(preds, labels, pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 / n_nodes * torch.mean(torch.sum(\n",
    "        1 + 2 * logvar - mu.pow(2) - logvar.exp().pow(2), 1))\n",
    "    return cost + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bcb4490-0d68-461e-b6da-3c2cc97a2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(Module):\n",
    "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout, act=torch.sigmoid):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.dropout(z, self.dropout, training=self.training)\n",
    "        adj = self.act(torch.mm(z, z.t()))\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fb9cacf-da0f-4a71-9aec-e80cc16e94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModelVAE(Module):\n",
    "    def __init__(self, input_feat_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "        super(GCNModelVAE, self).__init__()\n",
    "        self.gc1 = GCNLayer(input_feat_dim, hidden_dim1, dropout, act=F.relu)\n",
    "        self.gc2 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.gc3 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.dc = InnerProductDecoder(dropout, act=lambda x: x)\n",
    "\n",
    "    def encode(self, x, adj):\n",
    "        hidden1 = self.gc1(x, adj)\n",
    "        return self.gc2(hidden1, adj), self.gc3(hidden1, adj)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        mu, logvar = self.encode(x, adj)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.dc(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db0813be-93a0-4e77-b7fb-5ea27e912094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cbfd257-6523-4abf-ad8c-f1f7e07ca0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    \"\"\"\n",
    "    Function to build test set with 10% positive links\n",
    "    \"\"\"\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "    \n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "    \n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "    \n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "    \n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "def get_roc_score(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6de504c0-af1f-4ad4-9fa2-11fd4888e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed' : 42, \n",
    "    'epochs' : 200,\n",
    "    'hidden1' : 512,\n",
    "    'hidden2' : 128,\n",
    "    'lr' : 0.01,\n",
    "    'dropout' : 0.,\n",
    "    'dataset_str' : 'Cora'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "169431df-0d67-4afe-a269-7c61dfabebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    print(\"Using {} dataset\".format(args['dataset_str']))\n",
    "    adj, features = load_dataset(\n",
    "        osp.join('data', 'Planetoid'),\n",
    "        Planetoid,\n",
    "        args['dataset_str']\n",
    "    )\n",
    "    n_nodes, feat_dim = tuple(features.shape)\n",
    "\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "    \n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "    adj = adj_train\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    # adj_label = sparse_to_tuple(adj_label)\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    \n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "    model = GCNModelVAE(feat_dim, args['hidden1'], args['hidden2'], args['dropout'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    \n",
    "    hidden_emb = None\n",
    "    for epoch in range(args['epochs']):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        recovered, mu, logvar = model(features, adj_norm)\n",
    "        loss = loss_function(preds=recovered, labels=adj_label,\n",
    "                             mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                             norm=norm, pos_weight=pos_weight)\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        hidden_emb = mu.data.numpy()\n",
    "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "              \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "              \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "              )\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "021b753b-b877-4103-b128-23163d7ab71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cora dataset\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0001 train_loss= 4.46565 val_ap= 0.78170 time= 0.16106\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0002 train_loss= 4.33092 val_ap= 0.74532 time= 0.15368\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0003 train_loss= 3.78581 val_ap= 0.73536 time= 0.13901\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0004 train_loss= 3.00393 val_ap= 0.73406 time= 0.13440\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0005 train_loss= 2.38134 val_ap= 0.73475 time= 0.12675\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0006 train_loss= 1.70987 val_ap= 0.73465 time= 0.13209\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0007 train_loss= 1.14728 val_ap= 0.73567 time= 0.13005\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0008 train_loss= 0.89843 val_ap= 0.73488 time= 0.14420\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0009 train_loss= 0.94647 val_ap= 0.73441 time= 0.13457\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0010 train_loss= 0.90671 val_ap= 0.73533 time= 0.12799\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0011 train_loss= 0.83349 val_ap= 0.73806 time= 0.11855\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0012 train_loss= 0.83914 val_ap= 0.73926 time= 0.12597\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0013 train_loss= 0.86693 val_ap= 0.73916 time= 0.11891\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0014 train_loss= 0.85440 val_ap= 0.74021 time= 0.13124\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0015 train_loss= 0.84901 val_ap= 0.74104 time= 0.11828\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0016 train_loss= 0.85773 val_ap= 0.74014 time= 0.12043\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0017 train_loss= 0.86028 val_ap= 0.73943 time= 0.12467\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0018 train_loss= 0.85945 val_ap= 0.73917 time= 0.11938\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0019 train_loss= 0.85850 val_ap= 0.73985 time= 0.11555\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0020 train_loss= 0.85527 val_ap= 0.73944 time= 0.12275\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0021 train_loss= 0.85302 val_ap= 0.74027 time= 0.11720\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0022 train_loss= 0.84950 val_ap= 0.74296 time= 0.12363\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0023 train_loss= 0.84691 val_ap= 0.74560 time= 0.11905\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0024 train_loss= 0.84406 val_ap= 0.74785 time= 0.12576\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0025 train_loss= 0.84039 val_ap= 0.75004 time= 0.13016\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0026 train_loss= 0.83494 val_ap= 0.75226 time= 0.12316\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0027 train_loss= 0.82822 val_ap= 0.75466 time= 0.11775\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0028 train_loss= 0.82268 val_ap= 0.75656 time= 0.13278\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0029 train_loss= 0.81462 val_ap= 0.75826 time= 0.12472\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0030 train_loss= 0.80767 val_ap= 0.76047 time= 0.13821\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0031 train_loss= 0.79931 val_ap= 0.76211 time= 0.11991\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0032 train_loss= 0.79151 val_ap= 0.76396 time= 0.11746\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0033 train_loss= 0.78311 val_ap= 0.76623 time= 0.11869\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0034 train_loss= 0.77439 val_ap= 0.76871 time= 0.12583\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0035 train_loss= 0.76757 val_ap= 0.77002 time= 0.12057\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0036 train_loss= 0.75977 val_ap= 0.77253 time= 0.12661\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0037 train_loss= 0.75344 val_ap= 0.77326 time= 0.13035\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0038 train_loss= 0.74645 val_ap= 0.77445 time= 0.13766\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0039 train_loss= 0.74128 val_ap= 0.77505 time= 0.13042\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0040 train_loss= 0.73860 val_ap= 0.77464 time= 0.14011\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0041 train_loss= 0.73405 val_ap= 0.77466 time= 0.13540\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0042 train_loss= 0.73419 val_ap= 0.77555 time= 0.13224\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0043 train_loss= 0.73064 val_ap= 0.77903 time= 0.12755\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0044 train_loss= 0.72309 val_ap= 0.78589 time= 0.13309\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0045 train_loss= 0.71931 val_ap= 0.79570 time= 0.13437\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0046 train_loss= 0.70999 val_ap= 0.80571 time= 0.13629\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0047 train_loss= 0.69406 val_ap= 0.81223 time= 0.15466\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0048 train_loss= 0.68197 val_ap= 0.81350 time= 0.14430\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0049 train_loss= 0.66736 val_ap= 0.81092 time= 0.13878\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0050 train_loss= 0.66035 val_ap= 0.81048 time= 0.13875\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0051 train_loss= 0.65121 val_ap= 0.81365 time= 0.14922\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0052 train_loss= 0.64628 val_ap= 0.82029 time= 0.12959\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0053 train_loss= 0.63846 val_ap= 0.82796 time= 0.14938\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0054 train_loss= 0.62567 val_ap= 0.83791 time= 0.14654\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0055 train_loss= 0.61256 val_ap= 0.84772 time= 0.13332\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0056 train_loss= 0.60440 val_ap= 0.85745 time= 0.13875\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0057 train_loss= 0.59437 val_ap= 0.86416 time= 0.17202\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0058 train_loss= 0.58636 val_ap= 0.86862 time= 0.14643\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0059 train_loss= 0.58095 val_ap= 0.86897 time= 0.14158\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0060 train_loss= 0.57821 val_ap= 0.87160 time= 0.14849\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0061 train_loss= 0.57398 val_ap= 0.87298 time= 0.13084\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0062 train_loss= 0.56879 val_ap= 0.87620 time= 0.14508\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0063 train_loss= 0.56347 val_ap= 0.88403 time= 0.14720\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0064 train_loss= 0.55909 val_ap= 0.88905 time= 0.13345\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0065 train_loss= 0.55421 val_ap= 0.89132 time= 0.13208\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0066 train_loss= 0.55019 val_ap= 0.89321 time= 0.14156\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0067 train_loss= 0.54749 val_ap= 0.89432 time= 0.14153\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0068 train_loss= 0.54646 val_ap= 0.89354 time= 0.13161\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0069 train_loss= 0.54468 val_ap= 0.89590 time= 0.12861\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0070 train_loss= 0.54132 val_ap= 0.89951 time= 0.12805\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0071 train_loss= 0.53825 val_ap= 0.90298 time= 0.13578\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0072 train_loss= 0.53502 val_ap= 0.90534 time= 0.13089\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0073 train_loss= 0.53286 val_ap= 0.90783 time= 0.13181\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0074 train_loss= 0.52925 val_ap= 0.90935 time= 0.14463\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0075 train_loss= 0.52853 val_ap= 0.91038 time= 0.12931\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0076 train_loss= 0.52699 val_ap= 0.91092 time= 0.12899\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0077 train_loss= 0.52690 val_ap= 0.91257 time= 0.12796\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0078 train_loss= 0.52452 val_ap= 0.91427 time= 0.12818\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0079 train_loss= 0.52324 val_ap= 0.91417 time= 0.13017\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0080 train_loss= 0.52226 val_ap= 0.91540 time= 0.15001\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0081 train_loss= 0.52027 val_ap= 0.91685 time= 0.14100\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0082 train_loss= 0.51977 val_ap= 0.91813 time= 0.13805\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0083 train_loss= 0.51757 val_ap= 0.91929 time= 0.12712\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0084 train_loss= 0.51646 val_ap= 0.91969 time= 0.13025\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0085 train_loss= 0.51574 val_ap= 0.92073 time= 0.13673\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0086 train_loss= 0.51359 val_ap= 0.92198 time= 0.13315\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0087 train_loss= 0.51333 val_ap= 0.92314 time= 0.15199\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0088 train_loss= 0.51204 val_ap= 0.92398 time= 0.14149\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0089 train_loss= 0.51114 val_ap= 0.92494 time= 0.13300\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0090 train_loss= 0.51040 val_ap= 0.92591 time= 0.13488\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0091 train_loss= 0.50975 val_ap= 0.92691 time= 0.13754\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0092 train_loss= 0.50804 val_ap= 0.92724 time= 0.13175\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0093 train_loss= 0.50730 val_ap= 0.92690 time= 0.13659\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0094 train_loss= 0.50691 val_ap= 0.92810 time= 0.13504\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0095 train_loss= 0.50570 val_ap= 0.92785 time= 0.12950\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0096 train_loss= 0.50542 val_ap= 0.92853 time= 0.14916\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0097 train_loss= 0.50588 val_ap= 0.92854 time= 0.13602\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0098 train_loss= 0.50452 val_ap= 0.92852 time= 0.13348\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0099 train_loss= 0.50331 val_ap= 0.92904 time= 0.13348\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0100 train_loss= 0.50355 val_ap= 0.93005 time= 0.13626\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0101 train_loss= 0.50123 val_ap= 0.92984 time= 0.12562\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0102 train_loss= 0.50171 val_ap= 0.92808 time= 0.14405\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0103 train_loss= 0.50112 val_ap= 0.92969 time= 0.13121\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0104 train_loss= 0.50092 val_ap= 0.93114 time= 0.13811\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0105 train_loss= 0.49950 val_ap= 0.93048 time= 0.12723\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0106 train_loss= 0.49924 val_ap= 0.92893 time= 0.12922\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0107 train_loss= 0.49902 val_ap= 0.92920 time= 0.13129\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0108 train_loss= 0.49784 val_ap= 0.93113 time= 0.13558\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0109 train_loss= 0.49745 val_ap= 0.93154 time= 0.14039\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0110 train_loss= 0.49725 val_ap= 0.92978 time= 0.13145\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0111 train_loss= 0.49617 val_ap= 0.92945 time= 0.13780\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0112 train_loss= 0.49595 val_ap= 0.93182 time= 0.12825\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0113 train_loss= 0.49593 val_ap= 0.93261 time= 0.12740\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0114 train_loss= 0.49530 val_ap= 0.93094 time= 0.12880\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0115 train_loss= 0.49437 val_ap= 0.93072 time= 0.13114\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0116 train_loss= 0.49467 val_ap= 0.93294 time= 0.14101\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0117 train_loss= 0.49428 val_ap= 0.93277 time= 0.13234\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0118 train_loss= 0.49336 val_ap= 0.93102 time= 0.13109\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0119 train_loss= 0.49346 val_ap= 0.93155 time= 0.14293\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0120 train_loss= 0.49368 val_ap= 0.93335 time= 0.13235\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0121 train_loss= 0.49238 val_ap= 0.93308 time= 0.13356\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0122 train_loss= 0.49312 val_ap= 0.93101 time= 0.13242\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0123 train_loss= 0.49212 val_ap= 0.93153 time= 0.12969\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0124 train_loss= 0.49208 val_ap= 0.93345 time= 0.13008\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0125 train_loss= 0.49202 val_ap= 0.93401 time= 0.13491\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0126 train_loss= 0.49129 val_ap= 0.93257 time= 0.13610\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0127 train_loss= 0.49105 val_ap= 0.93226 time= 0.13457\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0128 train_loss= 0.48931 val_ap= 0.93436 time= 0.14939\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0129 train_loss= 0.49044 val_ap= 0.93445 time= 0.15028\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0130 train_loss= 0.48942 val_ap= 0.93312 time= 0.14542\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0131 train_loss= 0.48970 val_ap= 0.93347 time= 0.12999\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0132 train_loss= 0.48842 val_ap= 0.93455 time= 0.13561\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0133 train_loss= 0.48847 val_ap= 0.93492 time= 0.13322\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0134 train_loss= 0.48765 val_ap= 0.93469 time= 0.14731\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0135 train_loss= 0.48805 val_ap= 0.93457 time= 0.13347\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0136 train_loss= 0.48762 val_ap= 0.93515 time= 0.14000\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0137 train_loss= 0.48738 val_ap= 0.93521 time= 0.13762\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0138 train_loss= 0.48682 val_ap= 0.93499 time= 0.13190\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0139 train_loss= 0.48683 val_ap= 0.93477 time= 0.13706\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0140 train_loss= 0.48682 val_ap= 0.93464 time= 0.13012\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0141 train_loss= 0.48647 val_ap= 0.93511 time= 0.12786\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0142 train_loss= 0.48653 val_ap= 0.93525 time= 0.12803\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0143 train_loss= 0.48553 val_ap= 0.93434 time= 0.13189\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0144 train_loss= 0.48555 val_ap= 0.93419 time= 0.12731\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0145 train_loss= 0.48519 val_ap= 0.93565 time= 0.13144\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0146 train_loss= 0.48546 val_ap= 0.93547 time= 0.14480\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0147 train_loss= 0.48506 val_ap= 0.93427 time= 0.12921\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0148 train_loss= 0.48493 val_ap= 0.93372 time= 0.13669\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0149 train_loss= 0.48394 val_ap= 0.93554 time= 0.13261\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0150 train_loss= 0.48445 val_ap= 0.93567 time= 0.13336\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0151 train_loss= 0.48370 val_ap= 0.93477 time= 0.13125\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0152 train_loss= 0.48383 val_ap= 0.93491 time= 0.13172\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0153 train_loss= 0.48346 val_ap= 0.93549 time= 0.12646\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0154 train_loss= 0.48243 val_ap= 0.93505 time= 0.12776\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0155 train_loss= 0.48280 val_ap= 0.93499 time= 0.13142\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0156 train_loss= 0.48257 val_ap= 0.93588 time= 0.13198\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0157 train_loss= 0.48254 val_ap= 0.93572 time= 0.14289\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0158 train_loss= 0.48197 val_ap= 0.93530 time= 0.13235\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0159 train_loss= 0.48222 val_ap= 0.93564 time= 0.12930\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0160 train_loss= 0.48150 val_ap= 0.93580 time= 0.13693\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0161 train_loss= 0.48189 val_ap= 0.93585 time= 0.12795\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0162 train_loss= 0.48129 val_ap= 0.93578 time= 0.13225\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0163 train_loss= 0.48071 val_ap= 0.93617 time= 0.14070\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0164 train_loss= 0.48158 val_ap= 0.93676 time= 0.13075\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0165 train_loss= 0.48095 val_ap= 0.93645 time= 0.12914\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0166 train_loss= 0.48128 val_ap= 0.93658 time= 0.13443\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0167 train_loss= 0.48097 val_ap= 0.93705 time= 0.12757\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0168 train_loss= 0.47968 val_ap= 0.93717 time= 0.12920\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0169 train_loss= 0.47975 val_ap= 0.93668 time= 0.12782\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0170 train_loss= 0.48050 val_ap= 0.93685 time= 0.13089\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0171 train_loss= 0.47963 val_ap= 0.93724 time= 0.13354\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0172 train_loss= 0.47950 val_ap= 0.93799 time= 0.12987\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0173 train_loss= 0.47946 val_ap= 0.93747 time= 0.12986\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0174 train_loss= 0.47968 val_ap= 0.93739 time= 0.12870\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0175 train_loss= 0.47914 val_ap= 0.93721 time= 0.13176\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0176 train_loss= 0.47953 val_ap= 0.93747 time= 0.13282\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0177 train_loss= 0.47873 val_ap= 0.93731 time= 0.13166\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0178 train_loss= 0.47853 val_ap= 0.93771 time= 0.13013\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0179 train_loss= 0.47797 val_ap= 0.93780 time= 0.13062\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0180 train_loss= 0.47820 val_ap= 0.93730 time= 0.13358\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0181 train_loss= 0.47806 val_ap= 0.93670 time= 0.13589\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0182 train_loss= 0.47860 val_ap= 0.93694 time= 0.13006\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0183 train_loss= 0.47777 val_ap= 0.93689 time= 0.13390\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0184 train_loss= 0.47836 val_ap= 0.93664 time= 0.13021\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0185 train_loss= 0.47799 val_ap= 0.93721 time= 0.13403\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0186 train_loss= 0.47732 val_ap= 0.93721 time= 0.13063\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0187 train_loss= 0.47774 val_ap= 0.93627 time= 0.12708\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0188 train_loss= 0.47719 val_ap= 0.93566 time= 0.12427\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0189 train_loss= 0.47706 val_ap= 0.93625 time= 0.12615\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0190 train_loss= 0.47674 val_ap= 0.93658 time= 0.12985\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0191 train_loss= 0.47645 val_ap= 0.93645 time= 0.13050\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0192 train_loss= 0.47653 val_ap= 0.93518 time= 0.13497\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0193 train_loss= 0.47640 val_ap= 0.93515 time= 0.13111\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0194 train_loss= 0.47631 val_ap= 0.93634 time= 0.13461\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0195 train_loss= 0.47588 val_ap= 0.93653 time= 0.12419\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0196 train_loss= 0.47678 val_ap= 0.93535 time= 0.13343\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0197 train_loss= 0.47524 val_ap= 0.93448 time= 0.12534\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0198 train_loss= 0.47616 val_ap= 0.93472 time= 0.12937\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0199 train_loss= 0.47547 val_ap= 0.93556 time= 0.13158\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0200 train_loss= 0.47594 val_ap= 0.93512 time= 0.13586\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.92327412693669\n",
      "Test AP score: 0.9351636707963726\n"
     ]
    }
   ],
   "source": [
    "model = build_model(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona2vec",
   "language": "python",
   "name": "persona2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
