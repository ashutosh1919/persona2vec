{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2289997e-3125-4b93-afea-f3b410144552",
   "metadata": {},
   "source": [
    "## Exp 1: VGAE using GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5dd32f20-f0f2-47c2-9786-51dc1dc0dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid, KarateClub, PPI\n",
    "from torch_geometric.nn import GAE, VGAE, DenseGCNConv\n",
    "from torch_geometric.utils import to_networkx, to_dense_adj, dense_to_sparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.loss\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f3131901-cc81-4a9f-96f1-ee7e0e3d45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, dataset_cls, dataset_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device)\n",
    "    ])\n",
    "    obj = dataset_cls(data_dir, dataset_name, transform=transform)\n",
    "    network = to_networkx(obj[0])\n",
    "    return nx.adjacency_matrix(network), obj[0].x\n",
    "\n",
    "def load_dataset_pytorch(data_dir, dataset_cls, dataset_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device)\n",
    "    ])\n",
    "    obj = dataset_cls(data_dir, dataset_name, transform=transform)\n",
    "    return obj[0].x, obj[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13e84707-7e3c-4ef0-a643-43c9c7fb29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features = load_dataset(\n",
    "    osp.join('data', 'Planetoid'),\n",
    "    Planetoid,\n",
    "    'Cora'\n",
    ")\n",
    "\n",
    "# adj, features = load_dataset(osp.join('data', 'PPI'), PPI, 'PPI')\n",
    "\n",
    "# adj, features = load_dataset(\n",
    "#     osp.join('data', 'KarateClub'),\n",
    "#     KarateClub,\n",
    "#     'KarateClub'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ce9dd27-77dc-483d-a3c0-1958347f3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ccee4bd-9629-4315-aa57-62777ca7700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.coo_matrix(adj_train).shape\n",
    "# train_edges.transpose().shape\n",
    "# val_edges.transpose().shape\n",
    "# indices, ad = preprocess_graph(adj)\n",
    "# ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61082b84-2a75-45ae-a668-0ff7cef1ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device)\n",
    "])\n",
    "obj = Planetoid(osp.join('data', 'Planetoid'), 'Cora', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d5d78bb-ceed-4b5a-be89-f14d792b96c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dense_adj(obj[0].edge_index)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e375f5ed-5793-4702-9c62-ae52e27e68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Adj. Matrix: (2708, 2708)\n",
      "Shape of features: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Adj. Matrix: {}\".format(adj.shape))\n",
    "print(\"Shape of features: {}\".format(tuple(features.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3fb7e3e-6428-4514-ade2-296ab125de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0., act=F.relu):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input_tensor, adj):\n",
    "        input_tensor = F.dropout(input_tensor, self.dropout, self.training)\n",
    "        support = torch.mm(input_tensor, self.weight)\n",
    "        output_tensor = torch.spmm(adj, support)\n",
    "        output_tensor = self.act(output_tensor)\n",
    "        return output_tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "610a3545-32f0-4220-bbb2-42f7cfacdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, labels, mu, logvar, n_nodes, norm, pos_weight):\n",
    "    print(type(pos_weight))\n",
    "    cost = norm * F.binary_cross_entropy_with_logits(preds, labels, pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 / n_nodes * torch.mean(torch.sum(\n",
    "        1 + 2 * logvar - mu.pow(2) - logvar.exp().pow(2), 1))\n",
    "    return cost + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bcb4490-0d68-461e-b6da-3c2cc97a2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(Module):\n",
    "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout, act=torch.sigmoid):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.dropout(z, self.dropout, training=self.training)\n",
    "        adj = self.act(torch.mm(z, z.t()))\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3fb9cacf-da0f-4a71-9aec-e80cc16e94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModelVAE(Module):\n",
    "    def __init__(self, input_feat_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "        super(GCNModelVAE, self).__init__()\n",
    "        self.gc1 = GCNLayer(input_feat_dim, hidden_dim1, dropout, act=F.relu)\n",
    "        self.gc2 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.gc3 = GCNLayer(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.dc = InnerProductDecoder(dropout, act=lambda x: x)\n",
    "\n",
    "    def encode(self, x, adj):\n",
    "        hidden1 = self.gc1(x, adj)\n",
    "        return self.gc2(hidden1, adj), self.gc3(hidden1, adj)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        mu, logvar = self.encode(x, adj)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.dc(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db0813be-93a0-4e77-b7fb-5ea27e912094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6cbfd257-6523-4abf-ad8c-f1f7e07ca0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    \"\"\"\n",
    "    Function to build test set with 10% positive links\n",
    "    \"\"\"\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "    \n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "    \n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "    \n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "    \n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "    \n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "def get_roc_score(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6de504c0-af1f-4ad4-9fa2-11fd4888e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed' : 42, \n",
    "    'epochs' : 200,\n",
    "    'hidden1' : 64,\n",
    "    'hidden2' : 32,\n",
    "    'lr' : 0.05,\n",
    "    'dropout' : 0.,\n",
    "    'dataset_str' : 'Cora'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "169431df-0d67-4afe-a269-7c61dfabebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    print(\"Using {} dataset\".format(args['dataset_str']))\n",
    "    adj, features = load_dataset(\n",
    "        osp.join('data', 'Planetoid'),\n",
    "        Planetoid,\n",
    "        args['dataset_str']\n",
    "    )\n",
    "    # adj, features = load_dataset(\n",
    "    #     osp.join('data', 'KarateClub'),\n",
    "    #     KarateClub,\n",
    "    #     'KarateClub'\n",
    "    # )\n",
    "    # adj, features = load_dataset(\n",
    "    #     osp.join('data', 'PPI'),\n",
    "    #     PPI,\n",
    "    #     args['dataset_str']\n",
    "    # )\n",
    "    n_nodes, feat_dim = tuple(features.shape)\n",
    "    # print(n_nodes, feat_dim)\n",
    "\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    adj_orig = adj\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "    adj_orig.eliminate_zeros()\n",
    "    \n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "    adj = adj_train\n",
    "    \n",
    "    # Some preprocessing\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    # adj_label = sparse_to_tuple(adj_label)\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    \n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    \n",
    "    model = GCNModelVAE(feat_dim, args['hidden1'], args['hidden2'], args['dropout'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    \n",
    "    hidden_emb = None\n",
    "    # print(torch.unsqueeze(features, 0).shape, torch.unsqueeze(adj_norm, 0).shape)\n",
    "    for epoch in range(args['epochs']):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        recovered, mu, logvar = model(features, adj_norm)\n",
    "        loss = loss_function(preds=recovered, labels=adj_label,\n",
    "                             mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                             norm=norm, pos_weight=pos_weight)\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        hidden_emb = mu.data.numpy()\n",
    "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "              \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "              \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "              )\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "021b753b-b877-4103-b128-23163d7ab71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PPI dataset\n",
      "1767 50\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0001 train_loss= 2.40904 val_ap= 0.64316 time= 0.08004\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0002 train_loss= 1.62688 val_ap= 0.66939 time= 0.05691\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0003 train_loss= 0.95371 val_ap= 0.74573 time= 0.05341\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0004 train_loss= 0.82524 val_ap= 0.72390 time= 0.05387\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0005 train_loss= 0.77724 val_ap= 0.71155 time= 0.05547\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0006 train_loss= 0.77192 val_ap= 0.75474 time= 0.05134\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0007 train_loss= 0.73987 val_ap= 0.76720 time= 0.05319\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0008 train_loss= 0.71609 val_ap= 0.76920 time= 0.05331\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0009 train_loss= 0.72010 val_ap= 0.77744 time= 0.05425\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0010 train_loss= 0.71702 val_ap= 0.78937 time= 0.05236\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0011 train_loss= 0.70269 val_ap= 0.80072 time= 0.05045\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0012 train_loss= 0.69608 val_ap= 0.80724 time= 0.05440\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0013 train_loss= 0.69307 val_ap= 0.81087 time= 0.05390\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0014 train_loss= 0.68340 val_ap= 0.81171 time= 0.05292\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0015 train_loss= 0.67658 val_ap= 0.81081 time= 0.05241\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0016 train_loss= 0.66855 val_ap= 0.81176 time= 0.05401\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0017 train_loss= 0.66301 val_ap= 0.81297 time= 0.05635\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0018 train_loss= 0.65771 val_ap= 0.81244 time= 0.05232\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0019 train_loss= 0.65338 val_ap= 0.81135 time= 0.05736\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0020 train_loss= 0.64883 val_ap= 0.80839 time= 0.05761\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0021 train_loss= 0.64451 val_ap= 0.80669 time= 0.05773\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0022 train_loss= 0.64239 val_ap= 0.80625 time= 0.05685\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0023 train_loss= 0.63752 val_ap= 0.80818 time= 0.05692\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0024 train_loss= 0.63580 val_ap= 0.81142 time= 0.05907\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0025 train_loss= 0.62993 val_ap= 0.81248 time= 0.05771\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0026 train_loss= 0.62732 val_ap= 0.81455 time= 0.06016\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0027 train_loss= 0.62333 val_ap= 0.82099 time= 0.05836\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0028 train_loss= 0.62119 val_ap= 0.82623 time= 0.05723\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0029 train_loss= 0.61894 val_ap= 0.82395 time= 0.05768\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0030 train_loss= 0.61600 val_ap= 0.82355 time= 0.05592\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0031 train_loss= 0.61485 val_ap= 0.83113 time= 0.05685\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0032 train_loss= 0.61157 val_ap= 0.82899 time= 0.06731\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0033 train_loss= 0.60972 val_ap= 0.82650 time= 0.06979\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0034 train_loss= 0.60870 val_ap= 0.83092 time= 0.05316\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0035 train_loss= 0.60867 val_ap= 0.82822 time= 0.05433\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0036 train_loss= 0.60821 val_ap= 0.82438 time= 0.05788\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0037 train_loss= 0.60842 val_ap= 0.82790 time= 0.05397\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0038 train_loss= 0.60807 val_ap= 0.82852 time= 0.05558\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0039 train_loss= 0.60655 val_ap= 0.82573 time= 0.05277\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0040 train_loss= 0.60467 val_ap= 0.83015 time= 0.05604\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0041 train_loss= 0.60474 val_ap= 0.83429 time= 0.05617\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0042 train_loss= 0.60349 val_ap= 0.83019 time= 0.05506\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0043 train_loss= 0.60216 val_ap= 0.83267 time= 0.05341\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0044 train_loss= 0.60256 val_ap= 0.83590 time= 0.05615\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0045 train_loss= 0.60237 val_ap= 0.83279 time= 0.05464\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0046 train_loss= 0.60188 val_ap= 0.83315 time= 0.05322\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0047 train_loss= 0.60201 val_ap= 0.83766 time= 0.05458\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0048 train_loss= 0.59979 val_ap= 0.83453 time= 0.05597\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0049 train_loss= 0.59995 val_ap= 0.83421 time= 0.05821\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0050 train_loss= 0.60147 val_ap= 0.83762 time= 0.05498\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0051 train_loss= 0.59932 val_ap= 0.83642 time= 0.05503\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0052 train_loss= 0.59722 val_ap= 0.83631 time= 0.05761\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0053 train_loss= 0.59819 val_ap= 0.83900 time= 0.05561\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0054 train_loss= 0.59833 val_ap= 0.83521 time= 0.05706\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0055 train_loss= 0.59832 val_ap= 0.83510 time= 0.05604\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0056 train_loss= 0.59798 val_ap= 0.83832 time= 0.05664\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0057 train_loss= 0.59639 val_ap= 0.83492 time= 0.05636\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0058 train_loss= 0.59720 val_ap= 0.83912 time= 0.05513\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0059 train_loss= 0.59677 val_ap= 0.83712 time= 0.06191\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0060 train_loss= 0.59586 val_ap= 0.83772 time= 0.05788\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0061 train_loss= 0.59650 val_ap= 0.84251 time= 0.05759\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0062 train_loss= 0.59450 val_ap= 0.83104 time= 0.05687\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0063 train_loss= 0.59901 val_ap= 0.84733 time= 0.06019\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0064 train_loss= 0.59944 val_ap= 0.81914 time= 0.05558\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0065 train_loss= 0.59716 val_ap= 0.84812 time= 0.05906\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0066 train_loss= 0.59253 val_ap= 0.83531 time= 0.05383\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0067 train_loss= 0.59403 val_ap= 0.83257 time= 0.05401\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0068 train_loss= 0.59538 val_ap= 0.84834 time= 0.05439\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0069 train_loss= 0.59203 val_ap= 0.83260 time= 0.05804\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0070 train_loss= 0.59173 val_ap= 0.83819 time= 0.05580\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0071 train_loss= 0.59186 val_ap= 0.84486 time= 0.05466\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0072 train_loss= 0.59138 val_ap= 0.83409 time= 0.05581\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0073 train_loss= 0.59053 val_ap= 0.84037 time= 0.05410\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0074 train_loss= 0.59103 val_ap= 0.84531 time= 0.05202\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0075 train_loss= 0.59016 val_ap= 0.83434 time= 0.05263\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0076 train_loss= 0.58944 val_ap= 0.84555 time= 0.05559\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0077 train_loss= 0.58891 val_ap= 0.83835 time= 0.05655\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0078 train_loss= 0.58753 val_ap= 0.83540 time= 0.05387\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0079 train_loss= 0.58795 val_ap= 0.84721 time= 0.05361\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0080 train_loss= 0.58805 val_ap= 0.83342 time= 0.05496\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0081 train_loss= 0.58583 val_ap= 0.84541 time= 0.06141\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0082 train_loss= 0.58566 val_ap= 0.84200 time= 0.05341\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0083 train_loss= 0.58696 val_ap= 0.83654 time= 0.05514\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0084 train_loss= 0.58593 val_ap= 0.84630 time= 0.06270\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0085 train_loss= 0.58575 val_ap= 0.83298 time= 0.06025\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0086 train_loss= 0.58476 val_ap= 0.84056 time= 0.05534\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0087 train_loss= 0.58465 val_ap= 0.84345 time= 0.05558\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0088 train_loss= 0.58467 val_ap= 0.83449 time= 0.05927\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0089 train_loss= 0.58581 val_ap= 0.85044 time= 0.05510\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0090 train_loss= 0.58617 val_ap= 0.82956 time= 0.06662\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0091 train_loss= 0.58499 val_ap= 0.84911 time= 0.05438\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0092 train_loss= 0.58377 val_ap= 0.83507 time= 0.06397\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0093 train_loss= 0.58258 val_ap= 0.84219 time= 0.05788\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0094 train_loss= 0.58244 val_ap= 0.84450 time= 0.05648\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0095 train_loss= 0.58303 val_ap= 0.83322 time= 0.05867\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0096 train_loss= 0.58286 val_ap= 0.84908 time= 0.05662\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0097 train_loss= 0.58260 val_ap= 0.83655 time= 0.05752\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0098 train_loss= 0.58140 val_ap= 0.84743 time= 0.05482\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0099 train_loss= 0.58120 val_ap= 0.84513 time= 0.05464\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0100 train_loss= 0.58107 val_ap= 0.84043 time= 0.07176\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0101 train_loss= 0.58027 val_ap= 0.84821 time= 0.05567\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0102 train_loss= 0.57893 val_ap= 0.83759 time= 0.05421\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0103 train_loss= 0.58046 val_ap= 0.84772 time= 0.05831\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0104 train_loss= 0.58191 val_ap= 0.83619 time= 0.05733\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0105 train_loss= 0.58158 val_ap= 0.85165 time= 0.05651\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0106 train_loss= 0.58322 val_ap= 0.82971 time= 0.06874\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0107 train_loss= 0.58131 val_ap= 0.85492 time= 0.05713\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0108 train_loss= 0.58101 val_ap= 0.83799 time= 0.06542\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0109 train_loss= 0.57861 val_ap= 0.84815 time= 0.05543\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0110 train_loss= 0.57928 val_ap= 0.84909 time= 0.05771\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0111 train_loss= 0.58163 val_ap= 0.83104 time= 0.05525\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0112 train_loss= 0.58191 val_ap= 0.85560 time= 0.05782\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0113 train_loss= 0.57880 val_ap= 0.83740 time= 0.05487\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0114 train_loss= 0.57658 val_ap= 0.84547 time= 0.05368\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0115 train_loss= 0.57868 val_ap= 0.85510 time= 0.05380\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0116 train_loss= 0.58073 val_ap= 0.83142 time= 0.05579\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0117 train_loss= 0.57782 val_ap= 0.85598 time= 0.05705\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0118 train_loss= 0.57488 val_ap= 0.84664 time= 0.05483\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0119 train_loss= 0.57504 val_ap= 0.84067 time= 0.05503\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0120 train_loss= 0.57769 val_ap= 0.85514 time= 0.05768\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0121 train_loss= 0.57703 val_ap= 0.83707 time= 0.05938\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0122 train_loss= 0.57521 val_ap= 0.85219 time= 0.05457\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0123 train_loss= 0.57566 val_ap= 0.85608 time= 0.06627\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0124 train_loss= 0.57709 val_ap= 0.83784 time= 0.06100\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0125 train_loss= 0.57546 val_ap= 0.85549 time= 0.05633\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0126 train_loss= 0.57313 val_ap= 0.84847 time= 0.05548\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0127 train_loss= 0.57531 val_ap= 0.84496 time= 0.05665\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0128 train_loss= 0.57474 val_ap= 0.85649 time= 0.07040\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0129 train_loss= 0.57429 val_ap= 0.84392 time= 0.07804\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0130 train_loss= 0.57353 val_ap= 0.85163 time= 0.05664\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0131 train_loss= 0.57301 val_ap= 0.85225 time= 0.05351\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0132 train_loss= 0.57271 val_ap= 0.84614 time= 0.05560\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0133 train_loss= 0.57354 val_ap= 0.85615 time= 0.05609\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0134 train_loss= 0.57259 val_ap= 0.84271 time= 0.05511\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0135 train_loss= 0.57146 val_ap= 0.85094 time= 0.05460\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0136 train_loss= 0.57184 val_ap= 0.85439 time= 0.05676\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0137 train_loss= 0.57297 val_ap= 0.84013 time= 0.06057\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0138 train_loss= 0.57266 val_ap= 0.85719 time= 0.05648\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0139 train_loss= 0.57196 val_ap= 0.84499 time= 0.05486\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0140 train_loss= 0.57104 val_ap= 0.84994 time= 0.06241\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0141 train_loss= 0.57070 val_ap= 0.85342 time= 0.05787\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0142 train_loss= 0.57265 val_ap= 0.84552 time= 0.05486\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0143 train_loss= 0.57314 val_ap= 0.85688 time= 0.05731\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0144 train_loss= 0.57288 val_ap= 0.83882 time= 0.05847\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0145 train_loss= 0.57134 val_ap= 0.85772 time= 0.05898\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0146 train_loss= 0.56963 val_ap= 0.84740 time= 0.05607\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0147 train_loss= 0.56933 val_ap= 0.84959 time= 0.05645\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0148 train_loss= 0.56960 val_ap= 0.85554 time= 0.05659\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0149 train_loss= 0.56984 val_ap= 0.84306 time= 0.05588\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0150 train_loss= 0.56870 val_ap= 0.85414 time= 0.05389\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0151 train_loss= 0.56880 val_ap= 0.85065 time= 0.05299\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0152 train_loss= 0.56847 val_ap= 0.84947 time= 0.05554\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0153 train_loss= 0.56919 val_ap= 0.85296 time= 0.05788\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0154 train_loss= 0.57126 val_ap= 0.83868 time= 0.05809\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0155 train_loss= 0.56969 val_ap= 0.85985 time= 0.05313\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0156 train_loss= 0.56960 val_ap= 0.83806 time= 0.05678\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0157 train_loss= 0.56889 val_ap= 0.85719 time= 0.05696\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0158 train_loss= 0.56826 val_ap= 0.84447 time= 0.05729\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0159 train_loss= 0.56768 val_ap= 0.85255 time= 0.05612\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0160 train_loss= 0.56601 val_ap= 0.85283 time= 0.05821\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0161 train_loss= 0.56737 val_ap= 0.84946 time= 0.05695\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0162 train_loss= 0.56674 val_ap= 0.85149 time= 0.05291\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0163 train_loss= 0.56630 val_ap= 0.84654 time= 0.05373\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0164 train_loss= 0.56646 val_ap= 0.85786 time= 0.05419\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0165 train_loss= 0.56651 val_ap= 0.84299 time= 0.05852\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0166 train_loss= 0.56457 val_ap= 0.85539 time= 0.05603\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0167 train_loss= 0.56584 val_ap= 0.85081 time= 0.05458\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0168 train_loss= 0.56491 val_ap= 0.85070 time= 0.05826\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0169 train_loss= 0.56394 val_ap= 0.84971 time= 0.05585\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0170 train_loss= 0.56541 val_ap= 0.85266 time= 0.05882\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0171 train_loss= 0.56406 val_ap= 0.84898 time= 0.05390\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0172 train_loss= 0.56430 val_ap= 0.85270 time= 0.05518\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0173 train_loss= 0.56493 val_ap= 0.85070 time= 0.05438\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0174 train_loss= 0.56486 val_ap= 0.85741 time= 0.05434\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0175 train_loss= 0.56730 val_ap= 0.83143 time= 0.05337\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0176 train_loss= 0.57633 val_ap= 0.86623 time= 0.05690\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0177 train_loss= 0.59507 val_ap= 0.79350 time= 0.06147\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0178 train_loss= 0.56906 val_ap= 0.86204 time= 0.05379\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0179 train_loss= 0.57010 val_ap= 0.86426 time= 0.05226\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0180 train_loss= 0.58285 val_ap= 0.81978 time= 0.05702\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0181 train_loss= 0.56760 val_ap= 0.85631 time= 0.05503\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0182 train_loss= 0.57935 val_ap= 0.86325 time= 0.05410\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0183 train_loss= 0.57702 val_ap= 0.81710 time= 0.05890\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0184 train_loss= 0.56877 val_ap= 0.84138 time= 0.06165\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0185 train_loss= 0.58246 val_ap= 0.86909 time= 0.05373\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0186 train_loss= 0.56689 val_ap= 0.83963 time= 0.05599\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0187 train_loss= 0.57305 val_ap= 0.82500 time= 0.05568\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0188 train_loss= 0.56860 val_ap= 0.86008 time= 0.05416\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0189 train_loss= 0.56716 val_ap= 0.85977 time= 0.06169\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0190 train_loss= 0.56790 val_ap= 0.83170 time= 0.05494\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0191 train_loss= 0.56595 val_ap= 0.83932 time= 0.05693\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0192 train_loss= 0.56834 val_ap= 0.85838 time= 0.05625\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0193 train_loss= 0.56616 val_ap= 0.84982 time= 0.05730\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0194 train_loss= 0.56524 val_ap= 0.84556 time= 0.05389\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0195 train_loss= 0.56640 val_ap= 0.85433 time= 0.05459\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0196 train_loss= 0.56414 val_ap= 0.84971 time= 0.05344\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0197 train_loss= 0.56423 val_ap= 0.84418 time= 0.05418\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0198 train_loss= 0.56428 val_ap= 0.85315 time= 0.05402\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0199 train_loss= 0.56472 val_ap= 0.85324 time= 0.05276\n",
      "<class 'numpy.float64'>\n",
      "Epoch: 0200 train_loss= 0.56211 val_ap= 0.84358 time= 0.05619\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.8493546377325576\n",
      "Test AP score: 0.8557921616249844\n"
     ]
    }
   ],
   "source": [
    "model = build_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25271c8f-93d3-41b2-b20c-402ea69c95db",
   "metadata": {},
   "source": [
    "## Exp 2: VGAE using ChebConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07a33c06-aaf4-45a6-a777-973cef995732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn.models import InnerProductDecoder, VGAE\n",
    "from torch_geometric.nn.conv import GCNConv, ChebConv, SAGEConv, GraphConv, ResGatedGraphConv, GATConv, GATv2Conv\n",
    "from torch_geometric.utils import negative_sampling, remove_self_loops, add_self_loops\n",
    "\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.datasets import Planetoid, PPI\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c736437-6a6c-4a32-85ba-f51ef3df3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = PPI(os.path.join('data', 'PPI'), transform=T.NormalizeFeatures())\n",
    "# data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42cd4dfa-49e3-4d6f-af9e-cb690d0095e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81dd9140-ea6b-4ad1-b555-29fd1debc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.gcn_shared = GCNConv(in_channels, hidden_channels)\n",
    "        self.gcn_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.gcn_logvar = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gcn_shared(x, edge_index))\n",
    "        mu = self.gcn_mu(x, edge_index)\n",
    "        logvar = self.gcn_logvar(x, edge_index)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class DeepVGAE(VGAE):\n",
    "    def __init__(self, args):\n",
    "        super(DeepVGAE, self).__init__(encoder=GCNEncoder(args[\"enc_in_channels\"],\n",
    "                                                          args[\"enc_hidden_channels\"],\n",
    "                                                          args[\"enc_out_channels\"]),\n",
    "                                       decoder=InnerProductDecoder())\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        adj_pred = self.decoder.forward_all(z)\n",
    "        return adj_pred\n",
    "\n",
    "    def loss(self, x, pos_edge_index, all_edge_index):\n",
    "        z = self.encode(x, pos_edge_index)\n",
    "\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(z, pos_edge_index, sigmoid=True) + 1e-15).mean()\n",
    "\n",
    "        # Do not include self-loops in negative samples\n",
    "        all_edge_index_tmp, _ = remove_self_loops(all_edge_index)\n",
    "        all_edge_index_tmp, _ = add_self_loops(all_edge_index_tmp)\n",
    "\n",
    "        neg_edge_index = negative_sampling(all_edge_index_tmp, z.size(0), pos_edge_index.size(1))\n",
    "        neg_loss = -torch.log(1 - self.decoder(z, neg_edge_index, sigmoid=True) + 1e-15).mean()\n",
    "\n",
    "        kl_loss = 1 / x.size(0) * self.kl_loss()\n",
    "\n",
    "        return pos_loss + neg_loss + kl_loss\n",
    "\n",
    "    def single_test(self, x, train_pos_edge_index, test_pos_edge_index, test_neg_edge_index):\n",
    "        with torch.no_grad():\n",
    "            z = self.encode(x, train_pos_edge_index)\n",
    "        roc_auc_score, average_precision_score = self.test(z, test_pos_edge_index, test_neg_edge_index)\n",
    "        return roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48aeafc-2fd3-4b93-95c9-0ec8edf401d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset\": \"Cora\",\n",
    "    \"enc_in_channels\": 50,\n",
    "    \"enc_hidden_channels\": 256,\n",
    "    \"enc_out_channels\": 128,\n",
    "    \"lr\": 0.007,\n",
    "    \"epoch\": 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14bf9be-a016-4ae1-808f-df778527b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c8275e5-ac88-40be-9054-13649b313cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepVGAE(args).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "# os.makedirs(\"datasets\", exist_ok=True)\n",
    "dataset = PPI(os.path.join('data', 'PPI'), transform=T.NormalizeFeatures())  # Planetoid(os.path.join('data', 'Planetoid'), args[\"dataset\"], transform=T.NormalizeFeatures())\n",
    "data = dataset[0].to(device)\n",
    "# all_edge_index = data.edge_index\n",
    "# data = train_test_split_edges(data, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bae90ca-6438-4895-8aa4-1b9d8244e434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1767, 50])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8af783a0-1b22-4af1-8ac2-f384105d5511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 10.217914581298828 ROC_AUC: 0.723432794333311 Precision: 0.66171283152293\n",
      "Epoch 1 - Loss: 8.70817756652832 ROC_AUC: 0.7430112432784749 Precision: 0.7005583465165216\n",
      "Epoch 2 - Loss: 7.085495471954346 ROC_AUC: 0.768228776275053 Precision: 0.7454110402476584\n",
      "Epoch 3 - Loss: 5.992787837982178 ROC_AUC: 0.7930261001255643 Precision: 0.7764733317183106\n",
      "Epoch 4 - Loss: 4.111013889312744 ROC_AUC: 0.8057740417333628 Precision: 0.7888968437528647\n",
      "Epoch 5 - Loss: 2.9219131469726562 ROC_AUC: 0.7393255949927633 Precision: 0.6880493304700191\n",
      "Epoch 6 - Loss: 2.314441442489624 ROC_AUC: 0.7211187685111522 Precision: 0.6644903279129336\n",
      "Epoch 7 - Loss: 2.266303300857544 ROC_AUC: 0.7275419106863864 Precision: 0.6732092440395151\n",
      "Epoch 8 - Loss: 1.862894058227539 ROC_AUC: 0.7924013457427944 Precision: 0.760927412573196\n",
      "Epoch 9 - Loss: 1.604425311088562 ROC_AUC: 0.8077571911932444 Precision: 0.7892600742601363\n",
      "Epoch 10 - Loss: 1.6438645124435425 ROC_AUC: 0.8097025755063311 Precision: 0.7937051705354994\n",
      "Epoch 11 - Loss: 1.5899962186813354 ROC_AUC: 0.8114183017186019 Precision: 0.7956251263926144\n",
      "Epoch 12 - Loss: 1.4654966592788696 ROC_AUC: 0.8096598261269636 Precision: 0.7899467226355333\n",
      "Epoch 13 - Loss: 1.4974099397659302 ROC_AUC: 0.7989197634406541 Precision: 0.7802376445340923\n",
      "Epoch 14 - Loss: 1.5572673082351685 ROC_AUC: 0.7974290944991326 Precision: 0.7770172343198887\n",
      "Epoch 15 - Loss: 1.5263322591781616 ROC_AUC: 0.8071650260234451 Precision: 0.7872261478988966\n",
      "Epoch 16 - Loss: 1.4771257638931274 ROC_AUC: 0.8156472313546569 Precision: 0.8029511562947944\n",
      "Epoch 17 - Loss: 1.4706406593322754 ROC_AUC: 0.8197910456344832 Precision: 0.8113674456488894\n",
      "Epoch 18 - Loss: 1.478600025177002 ROC_AUC: 0.8223951154520794 Precision: 0.815712796463667\n",
      "Epoch 19 - Loss: 1.4488393068313599 ROC_AUC: 0.8230108598759693 Precision: 0.8156839033365374\n",
      "Epoch 20 - Loss: 1.4363442659378052 ROC_AUC: 0.8218905577547948 Precision: 0.8143020678371362\n",
      "Epoch 21 - Loss: 1.4498416185379028 ROC_AUC: 0.8215025544191933 Precision: 0.8142187229265178\n",
      "Epoch 22 - Loss: 1.4639248847961426 ROC_AUC: 0.8241349960221991 Precision: 0.8182881954986347\n",
      "Epoch 23 - Loss: 1.4434802532196045 ROC_AUC: 0.8291684958161201 Precision: 0.8267161055095391\n",
      "Epoch 24 - Loss: 1.4227991104125977 ROC_AUC: 0.8336376271218932 Precision: 0.8345186666035814\n",
      "Epoch 25 - Loss: 1.414703130722046 ROC_AUC: 0.8374003393112174 Precision: 0.8394898727388151\n",
      "Epoch 26 - Loss: 1.418257236480713 ROC_AUC: 0.8415836440491139 Precision: 0.8430770075209355\n",
      "Epoch 27 - Loss: 1.3945790529251099 ROC_AUC: 0.8445414026780665 Precision: 0.8446153549905417\n",
      "Epoch 28 - Loss: 1.3886531591415405 ROC_AUC: 0.8463301670676417 Precision: 0.8463662055054856\n",
      "Epoch 29 - Loss: 1.3876936435699463 ROC_AUC: 0.8482063472284792 Precision: 0.8479543044189396\n",
      "Epoch 30 - Loss: 1.382286548614502 ROC_AUC: 0.850081185480547 Precision: 0.8482047931704921\n",
      "Epoch 31 - Loss: 1.368699073791504 ROC_AUC: 0.8512388693460112 Precision: 0.8475837498335556\n",
      "Epoch 32 - Loss: 1.3612306118011475 ROC_AUC: 0.8521448494665913 Precision: 0.8481466619453564\n",
      "Epoch 33 - Loss: 1.352982997894287 ROC_AUC: 0.8530059714940237 Precision: 0.84923629414613\n",
      "Epoch 34 - Loss: 1.3441029787063599 ROC_AUC: 0.8532666851977878 Precision: 0.8493289358876395\n",
      "Epoch 35 - Loss: 1.340062141418457 ROC_AUC: 0.8516686635547164 Precision: 0.847116027210862\n",
      "Epoch 36 - Loss: 1.3291980028152466 ROC_AUC: 0.8501270020799587 Precision: 0.8455544933971004\n",
      "Epoch 37 - Loss: 1.3272476196289062 ROC_AUC: 0.8515156859550077 Precision: 0.8473181368284286\n",
      "Epoch 38 - Loss: 1.314224362373352 ROC_AUC: 0.8525809698166379 Precision: 0.8486672352420984\n",
      "Epoch 39 - Loss: 1.3171534538269043 ROC_AUC: 0.8499625224050839 Precision: 0.8459956357569121\n",
      "Epoch 40 - Loss: 1.3071444034576416 ROC_AUC: 0.8467074351330887 Precision: 0.8423892665121878\n",
      "Epoch 41 - Loss: 1.3004820346832275 ROC_AUC: 0.8474046525894046 Precision: 0.8433905825212947\n",
      "Epoch 42 - Loss: 1.297750473022461 ROC_AUC: 0.8498827746839326 Precision: 0.8466119264165736\n",
      "Epoch 43 - Loss: 1.2972298860549927 ROC_AUC: 0.8490203107477307 Precision: 0.8459146219471787\n",
      "Epoch 44 - Loss: 1.2955223321914673 ROC_AUC: 0.8461196790921028 Precision: 0.8431388257206196\n",
      "Epoch 45 - Loss: 1.2870447635650635 ROC_AUC: 0.845521379482215 Precision: 0.842926574580324\n",
      "Epoch 46 - Loss: 1.2802681922912598 ROC_AUC: 0.8484996501452138 Precision: 0.846866474367019\n",
      "Epoch 47 - Loss: 1.2847245931625366 ROC_AUC: 0.8504287398518149 Precision: 0.8493852808744224\n",
      "Epoch 48 - Loss: 1.2740741968154907 ROC_AUC: 0.8504153207641212 Precision: 0.8497659783382427\n",
      "Epoch 49 - Loss: 1.26859450340271 ROC_AUC: 0.8494100393946075 Precision: 0.8493376634589254\n",
      "Epoch 50 - Loss: 1.2763761281967163 ROC_AUC: 0.8495256352500264 Precision: 0.8501120818589241\n",
      "Epoch 51 - Loss: 1.2629315853118896 ROC_AUC: 0.852063376434165 Precision: 0.8533492093350488\n",
      "Epoch 52 - Loss: 1.2689354419708252 ROC_AUC: 0.8524975797716838 Precision: 0.854496100028141\n",
      "Epoch 53 - Loss: 1.265692949295044 ROC_AUC: 0.8518480959273069 Precision: 0.8547633721965964\n",
      "Epoch 54 - Loss: 1.260367751121521 ROC_AUC: 0.8526436561262927 Precision: 0.856321388968587\n",
      "Epoch 55 - Loss: 1.2603272199630737 ROC_AUC: 0.8546500014377593 Precision: 0.8587637437414072\n",
      "Epoch 56 - Loss: 1.2526193857192993 ROC_AUC: 0.8559159965110372 Precision: 0.8604849984464\n",
      "Epoch 57 - Loss: 1.2552151679992676 ROC_AUC: 0.8552745641192765 Precision: 0.860484427091191\n",
      "Epoch 58 - Loss: 1.255916714668274 ROC_AUC: 0.8547577375418148 Precision: 0.8604508419318739\n",
      "Epoch 59 - Loss: 1.2558521032333374 ROC_AUC: 0.8543873707214678 Precision: 0.8604885203240025\n",
      "Epoch 60 - Loss: 1.2542437314987183 ROC_AUC: 0.8561402869767756 Precision: 0.8623464724298769\n",
      "Epoch 61 - Loss: 1.2523515224456787 ROC_AUC: 0.8570109940668462 Precision: 0.8632951883424408\n",
      "Epoch 62 - Loss: 1.249699354171753 ROC_AUC: 0.8563862396840763 Precision: 0.8627464046558349\n",
      "Epoch 63 - Loss: 1.2428010702133179 ROC_AUC: 0.8569983417841636 Precision: 0.8633165039409603\n",
      "Epoch 64 - Loss: 1.2432475090026855 ROC_AUC: 0.8582098937016553 Precision: 0.8643605776926893\n",
      "Epoch 65 - Loss: 1.2449768781661987 ROC_AUC: 0.8566038206059676 Precision: 0.8628253937999518\n",
      "Epoch 66 - Loss: 1.2472622394561768 ROC_AUC: 0.8553449184790423 Precision: 0.8614375336968155\n",
      "Epoch 67 - Loss: 1.2455427646636963 ROC_AUC: 0.8572329841175512 Precision: 0.8632746316324302\n",
      "Epoch 68 - Loss: 1.2424769401550293 ROC_AUC: 0.8585181493161057 Precision: 0.8644289905141135\n",
      "Epoch 69 - Loss: 1.2432165145874023 ROC_AUC: 0.8597481045538633 Precision: 0.865483420383482\n",
      "Epoch 70 - Loss: 1.239725947380066 ROC_AUC: 0.858445302840054 Precision: 0.8641101112097963\n",
      "Epoch 71 - Loss: 1.238700270652771 ROC_AUC: 0.8567023550498901 Precision: 0.8625149598880191\n",
      "Epoch 72 - Loss: 1.2426151037216187 ROC_AUC: 0.8585513136328345 Precision: 0.8643121677939332\n",
      "Epoch 73 - Loss: 1.2334786653518677 ROC_AUC: 0.8623374133749965 Precision: 0.8680305381513013\n",
      "Epoch 74 - Loss: 1.236040711402893 ROC_AUC: 0.8609136481706908 Precision: 0.866903499921692\n",
      "Epoch 75 - Loss: 1.236008644104004 ROC_AUC: 0.8570847990491618 Precision: 0.8633702514708564\n",
      "Epoch 76 - Loss: 1.23521888256073 ROC_AUC: 0.8599691360983044 Precision: 0.8663500462107094\n",
      "Epoch 77 - Loss: 1.2270793914794922 ROC_AUC: 0.8642185777684056 Precision: 0.8703572674365245\n",
      "Epoch 78 - Loss: 1.2375624179840088 ROC_AUC: 0.8608212481668568 Precision: 0.8674953522019444\n",
      "Epoch 79 - Loss: 1.2312911748886108 ROC_AUC: 0.8586312530552387 Precision: 0.8653649611142349\n",
      "Epoch 80 - Loss: 1.235817790031433 ROC_AUC: 0.8602727908826884 Precision: 0.8669696321726079\n",
      "Epoch 81 - Loss: 1.2345291376113892 ROC_AUC: 0.8618742631483096 Precision: 0.8684910987002855\n",
      "Epoch 82 - Loss: 1.2320406436920166 ROC_AUC: 0.859815199992332 Precision: 0.8667671699886691\n",
      "Epoch 83 - Loss: 1.2295594215393066 ROC_AUC: 0.8585081808509618 Precision: 0.8656703655465199\n",
      "Epoch 84 - Loss: 1.2352502346038818 ROC_AUC: 0.8598895800784059 Precision: 0.8671360518171807\n",
      "Epoch 85 - Loss: 1.2293920516967773 ROC_AUC: 0.8609213162208015 Precision: 0.8682248134311678\n",
      "Epoch 86 - Loss: 1.2332100868225098 ROC_AUC: 0.8590560630313719 Precision: 0.866716534165042\n",
      "Epoch 87 - Loss: 1.2340432405471802 ROC_AUC: 0.856693536792263 Precision: 0.8645212289431117\n",
      "Epoch 88 - Loss: 1.2330453395843506 ROC_AUC: 0.8601165543616827 Precision: 0.8678516077167229\n",
      "Epoch 89 - Loss: 1.230268955230713 ROC_AUC: 0.8605448149603658 Precision: 0.8683024721577518\n",
      "Epoch 90 - Loss: 1.2305926084518433 ROC_AUC: 0.8598004389958689 Precision: 0.8677262541150825\n",
      "Epoch 91 - Loss: 1.2333974838256836 ROC_AUC: 0.8571904264394368 Precision: 0.8653983351062355\n",
      "Epoch 92 - Loss: 1.2276828289031982 ROC_AUC: 0.8620427685494925 Precision: 0.8697981849066104\n",
      "Epoch 93 - Loss: 1.2261930704116821 ROC_AUC: 0.8624855984433858 Precision: 0.8701801698520004\n",
      "Epoch 94 - Loss: 1.2286581993103027 ROC_AUC: 0.8585384696488992 Precision: 0.8667514493353893\n",
      "Epoch 95 - Loss: 1.2333078384399414 ROC_AUC: 0.8577049526018652 Precision: 0.8660219864650659\n",
      "Epoch 96 - Loss: 1.2301735877990723 ROC_AUC: 0.8622080150293783 Precision: 0.8700134090565668\n",
      "Epoch 97 - Loss: 1.2284798622131348 ROC_AUC: 0.8597657410691177 Precision: 0.8680076670685539\n",
      "Epoch 98 - Loss: 1.2264591455459595 ROC_AUC: 0.8589291568020396 Precision: 0.8674244563389448\n",
      "Epoch 99 - Loss: 1.229576587677002 ROC_AUC: 0.8606560016869711 Precision: 0.8689601513557212\n",
      "Epoch 100 - Loss: 1.2240817546844482 ROC_AUC: 0.8603477460725206 Precision: 0.8686560494522929\n",
      "Epoch 101 - Loss: 1.222589373588562 ROC_AUC: 0.8602649311313249 Precision: 0.8685029170995511\n",
      "Epoch 102 - Loss: 1.2297521829605103 ROC_AUC: 0.8601322738644097 Precision: 0.8682958060159549\n",
      "Epoch 103 - Loss: 1.226070523262024 ROC_AUC: 0.8596112298593872 Precision: 0.8678268250227144\n",
      "Epoch 104 - Loss: 1.224400281906128 ROC_AUC: 0.8612552598031228 Precision: 0.8692691412816758\n",
      "Epoch 105 - Loss: 1.2244739532470703 ROC_AUC: 0.8623153677309281 Precision: 0.870238250573277\n",
      "Epoch 106 - Loss: 1.2261971235275269 ROC_AUC: 0.8576459086160129 Precision: 0.8661175786624468\n",
      "Epoch 107 - Loss: 1.2241487503051758 ROC_AUC: 0.8589207219469178 Precision: 0.867335045886741\n",
      "Epoch 108 - Loss: 1.2193127870559692 ROC_AUC: 0.8632075453613091 Precision: 0.8711225483688014\n",
      "Epoch 109 - Loss: 1.2259254455566406 ROC_AUC: 0.8587999501576742 Precision: 0.8672988459535353\n",
      "Epoch 110 - Loss: 1.2229665517807007 ROC_AUC: 0.8558209126896645 Precision: 0.8644918960428516\n",
      "Epoch 111 - Loss: 1.2221119403839111 ROC_AUC: 0.8643059935396677 Precision: 0.8718553850108809\n",
      "Epoch 112 - Loss: 1.2238167524337769 ROC_AUC: 0.8617801378332008 Precision: 0.8696633154856581\n",
      "Epoch 113 - Loss: 1.2245736122131348 ROC_AUC: 0.8545919159581707 Precision: 0.8631150820524797\n",
      "Epoch 114 - Loss: 1.2309690713882446 ROC_AUC: 0.8620074955189833 Precision: 0.8698536076913269\n",
      "Epoch 115 - Loss: 1.222206473350525 ROC_AUC: 0.8636001495269773 Precision: 0.8711979335827492\n",
      "Epoch 116 - Loss: 1.2269530296325684 ROC_AUC: 0.853484457820932 Precision: 0.8620898317373594\n",
      "Epoch 117 - Loss: 1.2227721214294434 ROC_AUC: 0.8608020780415799 Precision: 0.8688380810325717\n",
      "Epoch 118 - Loss: 1.220954179763794 ROC_AUC: 0.8640149910379665 Precision: 0.8714773212260675\n",
      "Epoch 119 - Loss: 1.222429871559143 ROC_AUC: 0.8578192065485148 Precision: 0.8660740760334849\n",
      "Epoch 120 - Loss: 1.228028655052185 ROC_AUC: 0.8565091201871003 Precision: 0.8648493950942694\n",
      "Epoch 121 - Loss: 1.2227803468704224 ROC_AUC: 0.8650609130730669 Precision: 0.8722719894758975\n",
      "Epoch 122 - Loss: 1.2217555046081543 ROC_AUC: 0.8609866863479954 Precision: 0.8689161112045845\n",
      "Epoch 123 - Loss: 1.2142170667648315 ROC_AUC: 0.8530891698377249 Precision: 0.8616434910517057\n",
      "Epoch 124 - Loss: 1.2209872007369995 ROC_AUC: 0.8628686175464158 Precision: 0.8706541069519502\n",
      "Epoch 125 - Loss: 1.2223975658416748 ROC_AUC: 0.8635794457916783 Precision: 0.8712310341504146\n",
      "Epoch 126 - Loss: 1.2138596773147583 ROC_AUC: 0.855293159140795 Precision: 0.8638905864112892\n",
      "Epoch 127 - Loss: 1.2237070798873901 ROC_AUC: 0.8596652896126676 Precision: 0.8677716302621477\n",
      "Epoch 128 - Loss: 1.2234795093536377 ROC_AUC: 0.8614760996463112 Precision: 0.8694699332869833\n",
      "Epoch 129 - Loss: 1.219253420829773 ROC_AUC: 0.8588363733957002 Precision: 0.8672555173764996\n",
      "Epoch 130 - Loss: 1.2168593406677246 ROC_AUC: 0.8586157252537645 Precision: 0.8671604988067436\n",
      "Epoch 131 - Loss: 1.2158397436141968 ROC_AUC: 0.861783205053245 Precision: 0.8699626150966726\n",
      "Epoch 132 - Loss: 1.2206063270568848 ROC_AUC: 0.8575500579896289 Precision: 0.8662368411710086\n",
      "Epoch 133 - Loss: 1.2241672277450562 ROC_AUC: 0.8577337077897805 Precision: 0.8664201774667457\n",
      "Epoch 134 - Loss: 1.2141697406768799 ROC_AUC: 0.8639998466389978 Precision: 0.8717458833540751\n",
      "Epoch 135 - Loss: 1.2200323343276978 ROC_AUC: 0.8582938588503676 Precision: 0.866996919792263\n",
      "Epoch 136 - Loss: 1.2093555927276611 ROC_AUC: 0.8582179451542716 Precision: 0.8669352512590711\n",
      "Epoch 137 - Loss: 1.2130982875823975 ROC_AUC: 0.8614994871991488 Precision: 0.8698427140256909\n",
      "Epoch 138 - Loss: 1.2141886949539185 ROC_AUC: 0.8611396639477039 Precision: 0.8693825858550147\n",
      "Epoch 139 - Loss: 1.2191609144210815 ROC_AUC: 0.8559535699565797 Precision: 0.8647278693732584\n",
      "Epoch 140 - Loss: 1.2134997844696045 ROC_AUC: 0.8634141993117925 Precision: 0.8715032043955923\n",
      "Epoch 141 - Loss: 1.2125670909881592 ROC_AUC: 0.8604550987740707 Precision: 0.869009635510845\n",
      "Epoch 142 - Loss: 1.2076818943023682 ROC_AUC: 0.8574266023828466 Precision: 0.8663225046846352\n",
      "Epoch 143 - Loss: 1.2116336822509766 ROC_AUC: 0.8608427187071668 Precision: 0.8692372062477769\n",
      "Epoch 144 - Loss: 1.2100160121917725 ROC_AUC: 0.8606945336387772 Precision: 0.8689899721876673\n",
      "Epoch 145 - Loss: 1.2123123407363892 ROC_AUC: 0.8575182355816695 Precision: 0.8660767462216048\n",
      "Epoch 146 - Loss: 1.2124940156936646 ROC_AUC: 0.8628015221079469 Precision: 0.8706929522177942\n",
      "Epoch 147 - Loss: 1.215477466583252 ROC_AUC: 0.8584936115557515 Precision: 0.8669518525689738\n",
      "Epoch 148 - Loss: 1.215601921081543 ROC_AUC: 0.8586285692377 Precision: 0.8670857921192715\n",
      "Epoch 149 - Loss: 1.206714391708374 ROC_AUC: 0.8641610673925755 Precision: 0.8720238556365991\n",
      "Epoch 150 - Loss: 1.2089169025421143 ROC_AUC: 0.8588478754708662 Precision: 0.8673874708383429\n",
      "Epoch 151 - Loss: 1.2125996351242065 ROC_AUC: 0.857693642227952 Precision: 0.8663091304059725\n",
      "Epoch 152 - Loss: 1.2184759378433228 ROC_AUC: 0.8625653461645372 Precision: 0.8707099636623916\n",
      "Epoch 153 - Loss: 1.2138735055923462 ROC_AUC: 0.8611112921622943 Precision: 0.8694828036406627\n",
      "Epoch 154 - Loss: 1.212884545326233 ROC_AUC: 0.8620232150217102 Precision: 0.8704001992418303\n",
      "Epoch 155 - Loss: 1.210018515586853 ROC_AUC: 0.859599727784221 Precision: 0.8683700484180563\n",
      "Epoch 156 - Loss: 1.2135871648788452 ROC_AUC: 0.8638577960106969 Precision: 0.8719785478142067\n",
      "Epoch 157 - Loss: 1.2129344940185547 ROC_AUC: 0.8603385444123878 Precision: 0.8689251124660411\n",
      "Epoch 158 - Loss: 1.2070845365524292 ROC_AUC: 0.8635228939221118 Precision: 0.8717481406443097\n",
      "Epoch 159 - Loss: 1.2063586711883545 ROC_AUC: 0.860841760200903 Precision: 0.869523439333738\n",
      "Epoch 160 - Loss: 1.2036421298980713 ROC_AUC: 0.8615309262046027 Precision: 0.8701143885504914\n",
      "Epoch 161 - Loss: 1.2004506587982178 ROC_AUC: 0.8641265611670772 Precision: 0.872377631974377\n",
      "Epoch 162 - Loss: 1.212289810180664 ROC_AUC: 0.8531370951509168 Precision: 0.862377429848229\n",
      "Epoch 163 - Loss: 1.2061246633529663 ROC_AUC: 0.8649056350583251 Precision: 0.8730535130812871\n",
      "Epoch 164 - Loss: 1.2118109464645386 ROC_AUC: 0.8617026905270827 Precision: 0.8704685792431415\n",
      "Epoch 165 - Loss: 1.2051607370376587 ROC_AUC: 0.8563247035819379 Precision: 0.8657541962494113\n",
      "Epoch 166 - Loss: 1.2034664154052734 ROC_AUC: 0.8643615869029704 Precision: 0.8729820473399388\n",
      "Epoch 167 - Loss: 1.204899549484253 ROC_AUC: 0.8643753893931698 Precision: 0.8731937874351269\n",
      "Epoch 168 - Loss: 1.2109508514404297 ROC_AUC: 0.8522633208408016 Precision: 0.8627332585969647\n",
      "Epoch 169 - Loss: 1.212088704109192 ROC_AUC: 0.8667068600293303 Precision: 0.875234403969691\n",
      "Epoch 170 - Loss: 1.2140315771102905 ROC_AUC: 0.8632378341592463 Precision: 0.8724075293403343\n",
      "Epoch 171 - Loss: 1.2151302099227905 ROC_AUC: 0.8516506436369562 Precision: 0.8622528867799693\n",
      "Epoch 172 - Loss: 1.2054433822631836 ROC_AUC: 0.8708046660084923 Precision: 0.8783561194416942\n",
      "Epoch 173 - Loss: 1.2123448848724365 ROC_AUC: 0.8584499036701205 Precision: 0.8683825164510082\n",
      "Epoch 174 - Loss: 1.2000205516815186 ROC_AUC: 0.8582144945317218 Precision: 0.8680325156209512\n",
      "Epoch 175 - Loss: 1.2147822380065918 ROC_AUC: 0.8695777779907791 Precision: 0.8772904010293876\n",
      "Epoch 176 - Loss: 1.2117925882339478 ROC_AUC: 0.856910542610396 Precision: 0.8667210387696586\n",
      "Epoch 177 - Loss: 1.2059919834136963 ROC_AUC: 0.8582639534549359 Precision: 0.8678679769818152\n",
      "Epoch 178 - Loss: 1.2033346891403198 ROC_AUC: 0.8708706112394445 Precision: 0.8782714517989507\n",
      "Epoch 179 - Loss: 1.20976722240448 ROC_AUC: 0.8510490851057713 Precision: 0.8614811598540782\n",
      "Epoch 180 - Loss: 1.207789421081543 ROC_AUC: 0.8572832098457764 Precision: 0.8673814903012482\n",
      "Epoch 181 - Loss: 1.2070062160491943 ROC_AUC: 0.870805049410998 Precision: 0.87854714732799\n",
      "Epoch 182 - Loss: 1.2140692472457886 ROC_AUC: 0.8553217226274573 Precision: 0.8658877293086\n",
      "Epoch 183 - Loss: 1.204633355140686 ROC_AUC: 0.858064200749552 Precision: 0.8683579715816966\n",
      "Epoch 184 - Loss: 1.2092125415802002 ROC_AUC: 0.8681450028275935 Precision: 0.8766584255519201\n",
      "Epoch 185 - Loss: 1.2141977548599243 ROC_AUC: 0.858849792483394 Precision: 0.8693061702657495\n",
      "Epoch 186 - Loss: 1.2058287858963013 ROC_AUC: 0.8570826903353813 Precision: 0.8678357355505519\n",
      "Epoch 187 - Loss: 1.205464243888855 ROC_AUC: 0.868589366331509 Precision: 0.8771092353637535\n",
      "Epoch 188 - Loss: 1.2061258554458618 ROC_AUC: 0.8599022323610885 Precision: 0.8701837941026955\n",
      "Epoch 189 - Loss: 1.1984963417053223 ROC_AUC: 0.8603596315501922 Precision: 0.870714205399334\n",
      "Epoch 190 - Loss: 1.195523977279663 ROC_AUC: 0.8660134765980697 Precision: 0.8751654799071993\n",
      "Epoch 191 - Loss: 1.2043310403823853 ROC_AUC: 0.858670743513309 Precision: 0.8692637754012493\n",
      "Epoch 192 - Loss: 1.200361967086792 ROC_AUC: 0.8621528050685812 Precision: 0.8720429873411423\n",
      "Epoch 193 - Loss: 1.200918197631836 ROC_AUC: 0.8659488732758869 Precision: 0.8751625294565337\n",
      "Epoch 194 - Loss: 1.1971430778503418 ROC_AUC: 0.8611459900890452 Precision: 0.8711621083367752\n",
      "Epoch 195 - Loss: 1.2039580345153809 ROC_AUC: 0.8615581477824958 Precision: 0.8716382604916079\n",
      "Epoch 196 - Loss: 1.202458143234253 ROC_AUC: 0.8636369561675086 Precision: 0.8733943528928679\n",
      "Epoch 197 - Loss: 1.20111882686615 ROC_AUC: 0.862124816685677 Precision: 0.8722171546890436\n",
      "Epoch 198 - Loss: 1.1928552389144897 ROC_AUC: 0.863652100566477 Precision: 0.8735224329181724\n",
      "Epoch 199 - Loss: 1.1997700929641724 ROC_AUC: 0.8623935818420574 Precision: 0.8725717108917663\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args[\"epoch\"]):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.loss(data.x, data.train_pos_edge_index, all_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "        roc_auc, ap = model.single_test(data.x,\n",
    "                                        data.train_pos_edge_index,\n",
    "                                        data.test_pos_edge_index,\n",
    "                                        data.test_neg_edge_index)\n",
    "        print(\"Epoch {} - Loss: {} ROC_AUC: {} Precision: {}\".format(epoch, loss.cpu().item(), roc_auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629a737-030b-4f8a-80a9-7897349bde8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1466ee32-3b2a-4ec3-a128-7a22a01fb053",
   "metadata": {},
   "source": [
    "## NetworkX to torch_geometric.data.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa34485d-b390-46ca-b8a0-c978f7292c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2445c566-807f-426a-a542-e13589f6fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(input_file_path, weighted=False, directed=False):\n",
    "    \"\"\"\n",
    "    Reads the input network and return a networkx Graph object.\n",
    "    :param input_file_path: File path of input graph\n",
    "    :param weighted: weighted network(True) or unweighted network(False)\n",
    "    :param directed: directed network(True) or undirected network(False)\n",
    "    :return G: output network\n",
    "    \"\"\"\n",
    "    if weighted:\n",
    "        G = nx.read_edgelist(input_file_path, nodetype=str, create_using=nx.DiGraph(),)\n",
    "    else:\n",
    "        G = nx.read_edgelist(input_file_path, nodetype=str, create_using=nx.DiGraph())\n",
    "        for edge in G.edges():\n",
    "            G[edge[0]][edge[1]][\"weight\"] = 1\n",
    "\n",
    "    if not directed:\n",
    "        G = G.to_undirected()\n",
    "\n",
    "    return G\n",
    "\n",
    "def network_to_data(G):\n",
    "    data = from_networkx(G)\n",
    "    data.x = torch.from_numpy(csr_matrix.toarray(nx.adjacency_matrix(G)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c6942c-f9ec-443e-92ba-a10bd44a72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PPI(os.path.join('data', 'PPI'), transform=T.NormalizeFeatures())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1dd7e64-825e-4568-b920-cfa49f910b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32318])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea4952dd-361f-4005-a567-484df72f8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi_G = read_graph(\"graph/ppi.elist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3810edf7-e17e-46b0-b3fc-bbc1ccc34adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = network_to_data(ppi_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65adaa15-e354-47f0-b845-11a99f311d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi_G.node_attr_dict_factory(ppi_G.adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "028acaf7-d63e-4a3e-8448-68a65725f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = from_networkx(ppi_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cce41fe3-fc4d-4afd-967a-e9913d409c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x = csr_matrix.toarray(nx.adjacency_matrix(ppi_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d8dff3b-18a0-4d41-b163-cc12fa884a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(csr_matrix.toarray(nx.adjacency_matrix(ppi_G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ca7a57c-fad5-481e-b0e6-9a62db8d8595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3852"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2a027-1fb4-4e9a-87c7-5c79d62328f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona2vec",
   "language": "python",
   "name": "persona2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
